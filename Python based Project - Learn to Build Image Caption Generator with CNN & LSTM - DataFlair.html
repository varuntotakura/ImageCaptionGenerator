<!DOCTYPE html>
<!-- saved from url=(0083)https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/ -->
<html class="js" lang="en-US" slick-uniqueid="3"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="profile" href="http://gmpg.org/xfn/11"><link rel="pingback" href="https://data-flair.training/blogs/xmlrpc.php"><style id="wfc-style-fonts-body" data-origin="server">/* Setting : Default website font */ 
body {
font-family : Georgia,Georgia,serif!important;
}</style><link type="text/css" media="all" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/autoptimize_cc083d3c8a9f2a4a7b2a322a5f916f0e.css" rel="stylesheet"><title>Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair</title> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/osd.js.download"></script><script type="text/javascript" async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/recaptcha__en.js.download"></script><script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f.txt" id="google_shimpl"></script><script type="text/javascript" async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/recaptcha__en.js.download"></script><script>document.documentElement.className = document.documentElement.className.replace("no-js","js");</script> <meta name="description" content="Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN &amp; a type of RNN (LSTM) together."><meta name="robots" content="max-snippet:-1, max-image-preview:large, max-video-preview:-1"><link rel="canonical" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair"><meta property="og:description" content="Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN &amp; a type of RNN (LSTM) together."><meta property="og:url" content="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/"><meta property="og:site_name" content="DataFlair"><meta property="article:publisher" content="https://www.facebook.com/DataFlairWS/"><meta property="article:tag" content="Advanced python project"><meta property="article:tag" content="Image Caption Generator"><meta property="article:tag" content="python based project"><meta property="article:tag" content="Python data science project"><meta property="article:tag" content="Python project"><meta property="article:section" content="Python Tutorials"><meta property="article:published_time" content="2019-11-14T06:53:48+00:00"><meta property="article:modified_time" content="2020-01-07T04:45:02+00:00"><meta property="og:updated_time" content="2020-01-07T04:45:02+00:00"><meta property="fb:app_id" content="353250252066879"><meta property="og:image" content="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/python-project-image-caption-generator-with-CNN-and-LSTM.jpg"><meta property="og:image:secure_url" content="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/python-project-image-caption-generator-with-CNN-and-LSTM.jpg"><meta property="og:image:width" content="802"><meta property="og:image:height" content="420"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:description" content="Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN &amp; a type of RNN (LSTM) together."><meta name="twitter:title" content="Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair"><meta name="twitter:site" content="@DataFlairWS"><meta name="twitter:image" content="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/python-project-image-caption-generator-with-CNN-and-LSTM.jpg"><meta name="twitter:creator" content="@DataFlairWS"><link rel="dns-prefetch" href="https://s.w.org/"><link href="https://www.googletagmanager.com/" rel="preconnect"><link rel="alternate" type="application/rss+xml" title="DataFlair » Feed" href="https://data-flair.training/blogs/feed/"><link rel="alternate" type="application/rss+xml" title="DataFlair » Comments Feed" href="https://data-flair.training/blogs/comments/feed/"><link id="hu-user-gfont" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/css" rel="stylesheet" type="text/css"><link rel="alternate" type="application/rss+xml" title="DataFlair » Python based Project – Learn to Build Image Caption Generator with CNN &amp; LSTM Comments Feed" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/feed/"> <script>window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/data-flair.training\/blogs\/wp-includes\/js\/wp-emoji-release.min.js"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);</script><script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/wp-emoji-release.min.js.download" type="text/javascript" defer=""></script> <style>img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}</style><style id="hueman-main-style-inline-css">body { font-family:'Source Sans Pro', Arial, sans-serif;font-size:1.00rem }@media only screen and (min-width: 720px) {
        .nav > li { font-size:1.00rem; }
      }.container-inner { max-width: 2560px; }.sidebar .widget { padding-left: 20px; padding-right: 20px; padding-top: 20px; }::selection { background-color: #fea101; }
::-moz-selection { background-color: #fea101; }a,a+span.hu-external::after,.themeform label .required,#flexslider-featured .flex-direction-nav .flex-next:hover,#flexslider-featured .flex-direction-nav .flex-prev:hover,.post-hover:hover .post-title a,.post-title a:hover,.sidebar.s1 .post-nav li a:hover i,.content .post-nav li a:hover i,.post-related a:hover,.sidebar.s1 .widget_rss ul li a,#footer .widget_rss ul li a,.sidebar.s1 .widget_calendar a,#footer .widget_calendar a,.sidebar.s1 .alx-tab .tab-item-category a,.sidebar.s1 .alx-posts .post-item-category a,.sidebar.s1 .alx-tab li:hover .tab-item-title a,.sidebar.s1 .alx-tab li:hover .tab-item-comment a,.sidebar.s1 .alx-posts li:hover .post-item-title a,#footer .alx-tab .tab-item-category a,#footer .alx-posts .post-item-category a,#footer .alx-tab li:hover .tab-item-title a,#footer .alx-tab li:hover .tab-item-comment a,#footer .alx-posts li:hover .post-item-title a,.comment-tabs li.active a,.comment-awaiting-moderation,.child-menu a:hover,.child-menu .current_page_item > a,.wp-pagenavi a{ color: #fea101; }.themeform input[type="submit"],.themeform button[type="submit"],.sidebar.s1 .sidebar-top,.sidebar.s1 .sidebar-toggle,#flexslider-featured .flex-control-nav li a.flex-active,.post-tags a:hover,.sidebar.s1 .widget_calendar caption,#footer .widget_calendar caption,.author-bio .bio-avatar:after,.commentlist li.bypostauthor > .comment-body:after,.commentlist li.comment-author-admin > .comment-body:after{ background-color: #fea101; }.post-format .format-container { border-color: #fea101; }.sidebar.s1 .alx-tabs-nav li.active a,#footer .alx-tabs-nav li.active a,.comment-tabs li.active a,.wp-pagenavi a:hover,.wp-pagenavi a:active,.wp-pagenavi span.current{ border-bottom-color: #fea101!important; }.sidebar.s2 .post-nav li a:hover i,
.sidebar.s2 .widget_rss ul li a,
.sidebar.s2 .widget_calendar a,
.sidebar.s2 .alx-tab .tab-item-category a,
.sidebar.s2 .alx-posts .post-item-category a,
.sidebar.s2 .alx-tab li:hover .tab-item-title a,
.sidebar.s2 .alx-tab li:hover .tab-item-comment a,
.sidebar.s2 .alx-posts li:hover .post-item-title a { color: #65abf6; }
.sidebar.s2 .sidebar-top,.sidebar.s2 .sidebar-toggle,.post-comments,.jp-play-bar,.jp-volume-bar-value,.sidebar.s2 .widget_calendar caption{ background-color: #65abf6; }.sidebar.s2 .alx-tabs-nav li.active a { border-bottom-color: #65abf6; }
.post-comments span:before { border-right-color: #65abf6; }
      .search-expand,
              #nav-topbar.nav-container { background-color: #5ba1ec}@media only screen and (min-width: 720px) {
                #nav-topbar .nav ul { background-color: #5ba1ec; }
              }.is-scrolled #header .nav-container.desktop-sticky,
              .is-scrolled #header .search-expand { background-color: #5ba1ec; background-color: rgba(91,161,236,0.90) }.is-scrolled .topbar-transparent #nav-topbar.desktop-sticky .nav ul { background-color: #5ba1ec; background-color: rgba(91,161,236,0.95) }#header #nav-mobile { background-color: #5ba1ec; }.is-scrolled #header #nav-mobile { background-color: #5ba1ec; background-color: rgba(91,161,236,0.90) }#nav-header.nav-container, #main-header-search .search-expand { background-color: #5ba1ec; }
@media only screen and (min-width: 720px) {
  #nav-header .nav ul { background-color: #5ba1ec; }
}
        #footer-bottom { background-color: #022338; }.site-title a img { max-height: 70px; }body { background-color: #eaeaea; }</style><link rel="stylesheet" id="enlighter-local-css" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/EnlighterJS.min.css" type="text/css" media="all"> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/jquery.js.download"></script> <script>var ai_front = {"wp_ai":"4.9.14+2.5.7","insertion_before":"BEFORE","insertion_after":"AFTER","insertion_prepend":"PREPEND CONTENT","insertion_append":"APPEND CONTENT","insertion_replace_content":"REPLACE CONTENT","insertion_replace_element":"REPLACE ELEMENT","cancel":"Cancel","use":"Use","add":"Add","parent":"Parent","cancel_element_selection":"Cancel element selection","select_parent_element":"Select parent element","css_selector":"CSS selector","use_current_selector":"Use current selector","element":"ELEMENT","path":"PATH","selector":"SELECTOR","visible":"VISIBLE","hidden":"HIDDEN"};</script> <script>function ai_insert(a,e,h){var f=-1!=e.indexOf(":eq")?jQuery(e):document.querySelectorAll(e);Array.prototype.forEach.call(f,function(b,f){selector_string=b.hasAttribute("id")?"#"+b.getAttribute("id"):b.hasAttribute("class")?"."+b.getAttribute("class").replace(RegExp(" ","g"),"."):"";var d=document.createElement("div");d.innerHTML=h;var c=d.getElementsByClassName("ai-selector-counter")[0];null!=c&&(c.innerText=f+1);c=d.getElementsByClassName("ai-debug-name ai-main")[0];if(null!=c){var g="";"before"==
a?g=ai_front.insertion_before:"after"==a?g=ai_front.insertion_after:"prepend"==a?g=ai_front.insertion_prepend:"append"==a?g=ai_front.insertion_append:"replace-content"==a?g=ai_front.insertion_replace_content:"replace-element"==a&&(g=ai_front.insertion_replace_element);c.innerText=g+" "+e+" ("+b.tagName.toLowerCase()+selector_string+")"}c=document.createRange().createContextualFragment(d.innerHTML);"before"==a?b.parentNode.insertBefore(c,b):"after"==a?b.parentNode.insertBefore(c,b.nextSibling):"prepend"==
a?b.insertBefore(c,b.firstChild):"append"==a?b.insertBefore(c,null):"replace-content"==a?b.innerHTML=d.innerHTML:"replace-element"==a&&(b.parentNode.insertBefore(c,b),b.parentNode.removeChild(b))})}
function ai_insert_code(a){function e(a,b){return null==a?!1:a.classList?a.classList.contains(b):-1<(" "+a.className+" ").indexOf(" "+b+" ")}function h(a,b){null!=a&&(a.classList?a.classList.add(b):a.className+=" "+b)}function f(a,b){null!=a&&(a.classList?a.classList.remove(b):a.className=a.className.replace(new RegExp("(^|\\b)"+b.split(" ").join("|")+"(\\b|$)","gi")," "))}var b=e(a,"no-visibility-check")?!0:!!(a.offsetWidth||a.offsetHeight||a.getClientRects().length);a.getAttribute("data-block");
if(b){b=a.getAttribute("data-code");var k=a.getAttribute("data-insertion"),d=a.getAttribute("data-selector");null!=b&&(null!=k&&null!=d?document.querySelectorAll(d).length&&(ai_insert(k,d,b64d(b)),f(a,"ai-viewports")):(b=document.createRange().createContextualFragment(b64d(b)),a.parentNode.insertBefore(b,a.nextSibling),f(a,"ai-viewports")));a=a.getElementsByClassName("ai-check-block");"undefined"!=typeof a[0]&&a[0].parentNode.removeChild(a[0])}else b=a.previousElementSibling,e(b,"ai-debug-bar")&&
e(b,"ai-debug-script")&&(f(b,"ai-debug-script"),h(b,"ai-debug-viewport-invisible")),f(a,"ai-viewports")}function b64e(a){return btoa(encodeURIComponent(a).replace(/%([0-9A-F]{2})/g,function(a,h){return String.fromCharCode("0x"+h)}))}function b64d(a){return decodeURIComponent(atob(a).split("").map(function(a){return"%"+("00"+a.charCodeAt(0).toString(16)).slice(-2)}).join(""))};</script> <link rel="https://api.w.org/" href="https://data-flair.training/blogs/wp-json/"><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://data-flair.training/blogs/xmlrpc.php?rsd"><link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-includes/wlwmanifest.xml"><link rel="shortlink" href="https://data-flair.training/blogs/?p=72771"><link rel="alternate" type="application/json+oembed" href="https://data-flair.training/blogs/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fdata-flair.training%2Fblogs%2Fpython-based-project-image-caption-generator-cnn%2F"><link rel="alternate" type="text/xml+oembed" href="https://data-flair.training/blogs/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fdata-flair.training%2Fblogs%2Fpython-based-project-image-caption-generator-cnn%2F&amp;format=xml"> <script>function external_links_in_new_windows_loop() {
    if (!document.links) {
      document.links = document.getElementsByTagName('a');
    }
    var change_link = false;
    var force = '';
    var ignore = '';

    for (var t=0; t<document.links.length; t++) {
      var all_links = document.links[t];
      change_link = false;
      
      if(document.links[t].hasAttribute('onClick') == false) {
        // forced if the address starts with http (or also https), but does not link to the current domain
        if(all_links.href.search(/^http/) != -1 && all_links.href.search('data-flair.training') == -1 && all_links.href.search(/^#/) == -1) {
          // console.log('Changed ' + all_links.href);
          change_link = true;
        }
          
        if(force != '' && all_links.href.search(force) != -1) {
          // forced
          // console.log('force ' + all_links.href);
          change_link = true;
        }
        
        if(ignore != '' && all_links.href.search(ignore) != -1) {
          // console.log('ignore ' + all_links.href);
          // ignored
          change_link = false;
        }

        if(change_link == true) {
          // console.log('Changed ' + all_links.href);
          document.links[t].setAttribute('onClick', 'javascript:window.open(\''+all_links.href+'\'); return false;');
          document.links[t].removeAttribute('target');
        }
      }
    }
  }
  
  // Load
  function external_links_in_new_windows_load(func)
  {  
    var oldonload = window.onload;
    if (typeof window.onload != 'function'){
      window.onload = func;
    } else {
      window.onload = function(){
        oldonload();
        func();
      }
    }
  }

  external_links_in_new_windows_load(external_links_in_new_windows_loop);</script> <link rel="stylesheet" id="39552-css" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/autoptimize_single_f9d679723154a7a03462b2118239f171.css" type="text/css" media="all">
<!--[if lt IE 9]> <script src=https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/themes/hueman-pro/assets/front/js/ie/html5shiv-printshiv.min.js></script> <script src=https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/themes/hueman-pro/assets/front/js/ie/selectivizr.js></script> <![endif]--> <script type="application/ld+json" class="saswp-schema-markup-output">[{"@context":"https:\/\/schema.org","@graph":[{"@type":"EducationalOrganization","@id":"https:\/\/data-flair.training\/blogs#Organization","name":"DataFlair","url":"https:\/\/data-flair.training\/blogs","sameAs":[["https:\/\/www.facebook.com\/DataFlairWS\/"],["https:\/\/twitter.com\/DataFlairWS"],["https:\/\/www.instagram.com\/dataflair\/"],["https:\/\/www.youtube.com\/user\/DataFlairWS"],["https:\/\/www.linkedin.com\/company\/dataflair-web-services-pvt-ltd"],["https:\/\/in.pinterest.com\/DataFlair_Big_Data_Science\/"]],"logo":{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2016\/11\/DataFlair_Logo_.png","width":"150","height":"68"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","telephone":"+91-8451097879","url":"https:\/\/data-flair.training\/contact-us\/"}},{"@type":"WebSite","@id":"https:\/\/data-flair.training\/blogs#website","headline":"DataFlair","name":"DataFlair","description":"Learn Today. Lead Tomorrow.","url":"https:\/\/data-flair.training\/blogs","potentialAction":{"@type":"SearchAction","target":"https:\/\/data-flair.training\/blogs\/?s={search_term_string}","query-input":"required name=search_term_string"},"publisher":{"@id":"https:\/\/data-flair.training\/blogs#Organization"}},{"@context":"https:\/\/schema.org","@type":"WebPage","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage","name":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM","url":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","description":"Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN & a type of RNN (LSTM) together.","image":[{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/working-of-Deep-CNN-Python-project.png","width":"876","height":"318"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/LSTM-Cell-Structure-project-in-python.png","width":"1300","height":"853"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/Model-of-Image-Caption-Generator-python-project.png","width":"1648","height":"868"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/structure-python-data-science-project.png","width":"761","height":"429"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/jupyter-lab-advanced-python-project-1.jpg","width":"846","height":"517"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/token-file-project-in-python.png","width":"903","height":"440"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/descriptions-python-project-1.png","width":"1148","height":"639"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/save-descriptions-python-project.png","width":"690","height":"291"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/extracting_features.png","width":"805","height":"641"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/model-python-machine-learning-project.png","width":"829","height":"737"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-standing-on-rock.png","width":"1366","height":"522"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-girls-playing.png","width":"1366","height":"532"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-on-kayak.png","width":"1366","height":"531"},[{"@type":"ImageObject","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-1280x720.jpg","width":"1280","height":"720"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-640x480.jpg","width":"640","height":"480"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-300x300.jpg","width":"300","height":"300"}]],"primaryImageOfPage":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage"},"isPartOf":{"@id":"https:\/\/data-flair.training\/blogs#website"},"breadcrumb":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#breadcrumb"}},{"@type":"BreadcrumbList","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/data-flair.training\/blogs","name":"DataFlair"}},{"@type":"ListItem","position":2,"item":{"@id":"https:\/\/data-flair.training\/blogs\/category\/python\/","name":"Python Tutorials"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","name":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM"}}]},{"@type":"NewsArticle","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#newsarticle","url":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","headline":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM","mainEntityOfPage":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage","datePublished":"2019-11-14T12:23:48Z","dateModified":"2020-01-07T10:15:02Z","description":"Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN & a type of RNN (LSTM) together.","articleSection":"Python Tutorials","articleBody":"Project based on Python - Image Caption Generator\u00a0\r\n\r\nYou saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? Computer vision researchers worked on this a lot and they considered it impossible until now! With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image.\r\n\r\nThis is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together.\r\n\r\nBelow are some of the Python Data Science projects on which you can work later on:\r\n\r\n \tFake News Detection Python Project\r\n \tParkinson\u2019s Disease Detection Python Project\r\n \tColor Detection Python Project\r\n \tSpeech Emotion Recognition Python Project\r\n \tBreast Cancer Classification Python Project\r\n \tAge and Gender Detection Python Project\r\n \tHandwritten Digit Recognition Python Project\r\n \tChatbot Python Project\r\n \tDriver Drowsiness Detection Python Project\r\n \tTraffic Signs Recognition Python Project\r\n \tImage Caption Generator Python Project\r\n\r\nNow, let's quickly start the Python based project by defining the image caption generator.\r\nWhat is Image Caption Generator?\r\nImage caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English.\r\nImage Caption Generator with CNN - About the Python based Project\r\nThe objective of our project is to learn the concepts of a CNN and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM.\r\n\r\nIn this Python project, we will be implementing the caption generator using CNN (Convolutional Neural Networks) and LSTM (Long short term memory). The image features will be extracted from Xception which is a CNN model trained on the imagenet dataset and then we feed the features into the LSTM model which will be responsible for generating the image captions.\r\nThe Dataset of Python based Project\r\nFor the image caption generator, we will be using the Flickr_8K dataset. There are also other big datasets like Flickr_30K and MSCOCO dataset but it can take weeks just to train the network so we will be using a small Flickr8k dataset. The advantage of a huge dataset is that we can build better models.\r\n\r\nThanks to Jason Brownlee for providing a direct link to download the dataset (Size: 1GB).\r\n\r\n \tFlicker8k_Dataset\u00a0\r\n \tFlickr_8k_text\u00a0\r\n\r\nThe Flickr_8k_text folder contains file Flickr8k.token which is the main file of our dataset that contains image name and their respective captions separated by newline(\u201c\\n\u201d).\r\nPre-requisites\r\nThis project requires good knowledge of Deep learning, Python, working on Jupyter notebooks, Keras library, Numpy, and Natural language processing.\r\n\r\nMake sure you have installed all the following necessary libraries:\r\n\r\n \tpip install tensorflow\r\n \tkeras\r\n \tpillow\r\n \tnumpy\r\n \ttqdm\r\n \tjupyterlab\r\n\r\nImage Caption Generator - Python based Project\r\nWhat is CNN?\r\nConvolutional Neural networks are specialized deep neural networks which can process the data that has input shape like a 2D matrix. Images are easily represented as a 2D matrix and CNN is very useful in working with images.\r\n\r\nCNN is basically used for image classifications and identifying if an image is a bird, a plane or Superman, etc.\r\n\r\nIt scans images from left to right and top to bottom to pull out important features from the image and combines the feature to classify images. It can handle the images that have been translated, rotated, scaled and changes in perspective.\r\nPractise the important Python topics\r\nCheck out the 240+ Python Tutorials\r\n\r\nWhat is LSTM?\r\nLSTM stands for Long short term memory, they are a type of RNN (recurrent neural network) which is well suited for sequence prediction problems. Based on the previous text, we can predict what the next word will be. It has proven itself effective from the traditional RNN by overcoming the limitations of RNN which had short term memory. LSTM can carry out relevant information throughout the processing of inputs and with a forget gate, it discards non-relevant information.\r\n\r\nThis is what an LSTM cell looks like -\r\n\r\n\r\nImage Caption Generator Model\r\nSo, to make our image caption generator model, we will be merging these architectures. It is also called a CNN-RNN model.\r\n\r\n \tCNN is used for extracting features from the image. We will use the pre-trained model Xception.\r\n \tLSTM will use the information from CNN to help generate a description of the image.\r\n\r\n\r\n\r\nProject File Structure\r\nDownloaded from dataset:\r\n\r\n \tFlicker8k_Dataset - Dataset folder which contains 8091 images.\r\n \tFlickr_8k_text - Dataset folder which contains text files and captions of images.\r\n\r\nThe below files will be created by us while making the project.\r\n\r\n \tModels - It will contain our trained models.\r\n \tDescriptions.txt - This text file contains all image names and their captions after preprocessing.\r\n \tFeatures.p - Pickle object that contains an image and their feature vector extracted from the Xception pre-trained CNN model.\r\n \tTokenizer.p - Contains tokens mapped with an index value.\r\n \tModel.png - Visual representation of dimensions of our project.\r\n \tTesting_caption_generator.py - Python file for generating a caption of any image.\r\n \tTraining_caption_generator.ipynb - Jupyter notebook in which we train and build our image caption generator.\r\n\r\nYou can download all the files from the link:\r\n\r\nImage Caption Generator - Python Project Files\r\n\r\n\r\nWant to become a Python expert?\r\nEnroll for the Certified Python Training Course\r\n\r\nBuilding the Python based Project\r\nLet\u2019s start by initializing the jupyter notebook server by typing jupyter lab in the console of your project folder. It will open up the interactive Python notebook where you can run your code. Create a Python3 notebook and name it training_caption_generator.ipynb\r\n\r\n1. First, we import all the necessary packages\r\nimport string\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport os\r\nfrom pickle import dump, load\r\nimport numpy as np\r\n\r\nfrom keras.applications.xception import Xception, preprocess_input\r\nfrom keras.preprocessing.image import load_img, img_to_array\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.utils import to_categorical\r\nfrom keras.layers.merge import add\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout\r\n\r\n# small library for seeing the progress of loops.\r\nfrom tqdm import tqdm_notebook as tqdm\r\ntqdm().pandas()\r\n2. Getting and performing data cleaning\r\n\r\nThe main text file which contains all image captions is Flickr8k.token in our Flickr_8k_text folder.\r\n\r\nHave a look at the file -\r\n\r\nThe format of our file is image and caption separated by a new line (\u201c\\n\u201d).\r\n\r\nEach image has 5 captions and we can see that #(0 to 5)number is assigned for each caption.\r\n\r\nWe will define 5 functions:\r\n\r\n \tload_doc( filename ) - For loading the document file and reading the contents inside the file into a string.\r\n \tall_img_captions( filename ) - This function will create a descriptions dictionary that maps images with a list of 5 captions. The descriptions dictionary will look something like this:\r\n\r\n\r\n\r\n\r\n \tcleaning_text( descriptions) - This function takes all descriptions and performs data cleaning. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers.\r\nSo, a caption like \u201cA man riding on a three-wheeled wheelchair\u201d will be transformed into \u201cman riding on three wheeled wheelchair\u201d\r\n \ttext_vocabulary( descriptions ) - This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.\r\n \tsave_descriptions( descriptions, filename ) - This function will create a list of all the descriptions that have been preprocessed and store them into a file. We will create a descriptions.txt file to store all the captions. It will look something like this:\r\n\r\n\r\nCode :\r\n# Loading a text file into memory\r\ndef load_doc(filename):\r\n    # Opening the file as read only\r\n    file = open(filename, 'r')\r\n    text = file.read()\r\n    file.close()\r\n    return text\r\n\r\n# get all imgs with their captions\r\ndef all_img_captions(filename):\r\n    file = load_doc(filename)\r\n    captions = file.split('\\n')\r\n    descriptions ={}\r\n    for caption in captions[:-1]:\r\n        img, caption = caption.split('\\t')\r\n        if img[:-2] not in descriptions:\r\n            descriptions[img[:-2]] = \r\n        else:\r\n            descriptions[img[:-2]].append(caption)\r\n    return descriptions\r\n\r\n#Data cleaning- lower casing, removing puntuations and words containing numbers\r\ndef cleaning_text(captions):\r\n    table = str.maketrans('','',string.punctuation)\r\n    for img,caps in captions.items():\r\n        for i,img_caption in enumerate(caps):\r\n\r\n            img_caption.replace(\"-\",\" \")\r\n            desc = img_caption.split()\r\n\r\n            #converts to lowercase\r\n            desc = [word.lower() for word in desc]\r\n            #remove punctuation from each token\r\n            desc = [word.translate(table) for word in desc]\r\n            #remove hanging 's and a \r\n            desc = [word for word in desc if(len(word)&gt;1)]\r\n            #remove tokens with numbers in them\r\n            desc = [word for word in desc if(word.isalpha())]\r\n            #convert back to string\r\n\r\n            img_caption = ' '.join(desc)\r\n            captions[img][i]= img_caption\r\n    return captions\r\n\r\ndef text_vocabulary(descriptions):\r\n    # build vocabulary of all unique words\r\n    vocab = set()\r\n\r\n    for key in descriptions.keys():\r\n        [vocab.update(d.split()) for d in descriptions[key]]\r\n\r\n    return vocab\r\n\r\n#All descriptions in one file \r\ndef save_descriptions(descriptions, filename):\r\n    lines = list()\r\n    for key, desc_list in descriptions.items():\r\n        for desc in desc_list:\r\n            lines.append(key + '\\t' + desc )\r\n    data = \"\\n\".join(lines)\r\n    file = open(filename,\"w\")\r\n    file.write(data)\r\n    file.close()\r\n\r\n\r\n# Set these path according to project folder in you system\r\ndataset_text = \"D:\\dataflair projects\\Project - Image Caption Generator\\Flickr_8k_text\"\r\ndataset_images = \"D:\\dataflair projects\\Project - Image Caption Generator\\Flicker8k_Dataset\"\r\n\r\n#we prepare our text data\r\nfilename = dataset_text + \"\/\" + \"Flickr8k.token.txt\"\r\n#loading the file that contains all data\r\n#mapping them into descriptions dictionary img to 5 captions\r\ndescriptions = all_img_captions(filename)\r\nprint(\"Length of descriptions =\" ,len(descriptions))\r\n\r\n#cleaning the descriptions\r\nclean_descriptions = cleaning_text(descriptions)\r\n\r\n#building vocabulary \r\nvocabulary = text_vocabulary(clean_descriptions)\r\nprint(\"Length of vocabulary = \", len(vocabulary))\r\n\r\n#saving each description to file \r\nsave_descriptions(clean_descriptions, \"descriptions.txt\")\r\n3. Extracting the feature vector from all images\u00a0\r\n\r\nThis technique is also called transfer learning, we don\u2019t have to do everything on our own, we use the pre-trained model that have been already trained on large datasets and extract the features from these models and use them for our tasks. We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. We can directly import this model from the keras.applications . Make sure you are connected to the internet as the weights get automatically downloaded. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.\r\n\r\nmodel = Xception( include_top=False, pooling='avg' )\r\n\r\nThe function extract_features() will extract features for all images and we will map image names with their respective feature array. Then we will dump the features dictionary into a \u201cfeatures.p\u201d pickle file.\r\n\r\nCode:\r\ndef extract_features(directory):\r\n        model = Xception( include_top=False, pooling='avg' )\r\n        features = {}\r\n        for img in tqdm(os.listdir(directory)):\r\n            filename = directory + \"\/\" + img\r\n            image = Image.open(filename)\r\n            image = image.resize((299,299))\r\n            image = np.expand_dims(image, axis=0)\r\n            #image = preprocess_input(image)\r\n            image = image\/127.5\r\n            image = image - 1.0\r\n\r\n            feature = model.predict(image)\r\n            features[img] = feature\r\n        return features\r\n\r\n#2048 feature vector\r\nfeatures = extract_features(dataset_images)\r\ndump(features, open(\"features.p\",\"wb\"))\r\n\r\nThis process can take a lot of time depending on your system. I am using an Nvidia 1050 GPU for training purpose so it took me around 7 minutes for performing this task. However, if you are using CPU then this process might take 1-2 hours. You can comment out the code and directly load the features from our pickle file.\r\nfeatures = load(open(\"features.p\",\"rb\"))\r\n4. Loading dataset for Training the model\r\n\r\nIn our Flickr_8k_test folder, we have Flickr_8k.trainImages.txt file that contains a list of 6000 image names that we will use for training.\r\n\r\nFor loading the training dataset, we need more functions:\r\n\r\n \tload_photos( filename ) - This will load the text file in a string and will return the list of image names.\r\n \tload_clean_descriptions( filename, photos ) - This function will create a dictionary that contains captions for each photo from the list of photos. We also append the &lt;start&gt; and &lt;end&gt; identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.\r\n \tload_features(photos) - This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model.\r\n\r\nCode :\r\n#load the data \r\ndef load_photos(filename):\r\n    file = load_doc(filename)\r\n    photos = file.split(\"\\n\")[:-1]\r\n    return photos\r\n\r\n\r\ndef load_clean_descriptions(filename, photos): \r\n    #loading clean_descriptions\r\n    file = load_doc(filename)\r\n    descriptions = {}\r\n    for line in file.split(\"\\n\"):\r\n\r\n        words = line.split()\r\n        if len(words)&lt;1 :\r\n            continue\r\n\r\n        image, image_caption = words[0], words[1:]\r\n\r\n        if image in photos:\r\n            if image not in descriptions:\r\n                descriptions[image] = []\r\n            desc = '&lt;start&gt; ' + \" \".join(image_caption) + ' &lt;end&gt;'\r\n            descriptions[image].append(desc)\r\n\r\n    return descriptions\r\n\r\n\r\ndef load_features(photos):\r\n    #loading all features\r\n    all_features = load(open(\"features.p\",\"rb\"))\r\n    #selecting only needed features\r\n    features = {k:all_features[k] for k in photos}\r\n    return features\r\n\r\n\r\nfilename = dataset_text + \"\/\" + \"Flickr_8k.trainImages.txt\"\r\n\r\n#train = loading_data(filename)\r\ntrain_imgs = load_photos(filename)\r\ntrain_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\r\ntrain_features = load_features(train_imgs)\r\n5. Tokenizing the vocabulary\u00a0\r\n\r\nComputers don\u2019t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a \u201ctokenizer.p\u201d pickle file.\r\n\r\nCode:\r\n#converting dictionary to clean list of descriptions\r\ndef dict_to_list(descriptions):\r\n    all_desc = []\r\n    for key in descriptions.keys():\r\n        [all_desc.append(d) for d in descriptions[key]]\r\n    return all_desc\r\n\r\n#creating tokenizer class \r\n#this will vectorise text corpus\r\n#each integer will represent token in dictionary\r\n\r\nfrom keras.preprocessing.text import Tokenizer\r\n\r\ndef create_tokenizer(descriptions):\r\n    desc_list = dict_to_list(descriptions)\r\n    tokenizer = Tokenizer()\r\n    tokenizer.fit_on_texts(desc_list)\r\n    return tokenizer\r\n\r\n# give each word an index, and store that into tokenizer.p pickle file\r\ntokenizer = create_tokenizer(train_descriptions)\r\ndump(tokenizer, open('tokenizer.p', 'wb'))\r\nvocab_size = len(tokenizer.word_index) + 1\r\nvocab_size\r\nOur vocabulary contains 7577 words.\r\n\r\nWe calculate the maximum length of the descriptions. This is important for deciding the model structure parameters. Max_length of description is 32.\r\n#calculate maximum length of descriptions\r\ndef max_length(descriptions):\r\n    desc_list = dict_to_list(descriptions)\r\n    return max(len(d.split()) for d in desc_list)\r\n    \r\nmax_length = max_length(descriptions)\r\nmax_length\r\n6. Create Data generator\r\n\r\nLet us first see how the input and output of our model will look like. To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\r\n\r\nThe generator will yield the input and output sequence.\r\n\r\nFor example:\r\n\r\nThe input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict.\r\n\r\n\r\n\r\nx1(feature vector)\r\nx2(Text sequence)\r\ny(word to predict)\r\n\r\n\r\nfeature\r\nstart,\r\ntwo\r\n\r\n\r\nfeature\r\nstart, two\r\ndogs\r\n\r\n\r\nfeature\r\nstart, two, dogs\r\ndrink\r\n\r\n\r\nfeature\r\nstart, two, dogs, drink\r\nwater\r\n\r\n\r\nfeature\r\nstart, two, dogs, drink, water\r\nend\r\n\r\n\r\n\r\n#create input-output sequence pairs from the image description.\r\n\r\n#data generator, used by model.fit_generator()\r\ndef data_generator(descriptions, features, tokenizer, max_length):\r\n    while 1:\r\n        for key, description_list in descriptions.items():\r\n            #retrieve photo features\r\n            feature = features[key][0]\r\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\r\n            yield [[input_image, input_sequence], output_word]\r\n\r\ndef create_sequences(tokenizer, max_length, desc_list, feature):\r\n    X1, X2, y = list(), list(), list()\r\n    # walk through each description for the image\r\n    for desc in desc_list:\r\n        # encode the sequence\r\n        seq = tokenizer.texts_to_sequences([desc])[0]\r\n        # split one sequence into multiple X,y pairs\r\n        for i in range(1, len(seq)):\r\n            # split into input and output pair\r\n            in_seq, out_seq = seq[:i], seq[i]\r\n            # pad input sequence\r\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n            # encode output sequence\r\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n            # store\r\n            X1.append(feature)\r\n            X2.append(in_seq)\r\n            y.append(out_seq)\r\n    return np.array(X1), np.array(X2), np.array(y)\r\n\r\n#You can check the shape of the input and output for your model\r\n[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\r\na.shape, b.shape, c.shape\r\n#((47, 2048), (47, 32), (47, 7577))\r\n7. Defining the CNN-RNN model\r\n\r\nTo define the structure of the model, we will be using the Keras Model from Functional API. It will consist of three major parts:\r\n\r\n \tFeature Extractor - The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.\r\n \tSequence Processor - An embedding layer will handle the textual input, followed by the LSTM layer.\r\n \tDecoder - By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size.\r\n\r\nVisual representation of the final model is given below -\r\n\r\n\r\nfrom keras.utils import plot_model\r\n\r\n# define the captioning model\r\ndef define_model(vocab_size, max_length):\r\n\r\n    # features from the CNN model squeezed from 2048 to 256 nodes\r\n    inputs1 = Input(shape=(2048,))\r\n    fe1 = Dropout(0.5)(inputs1)\r\n    fe2 = Dense(256, activation='relu')(fe1)\r\n\r\n    # LSTM sequence model\r\n    inputs2 = Input(shape=(max_length,))\r\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\r\n    se2 = Dropout(0.5)(se1)\r\n    se3 = LSTM(256)(se2)\r\n\r\n    # Merging both models\r\n    decoder1 = add([fe2, se3])\r\n    decoder2 = Dense(256, activation='relu')(decoder1)\r\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n\r\n    # tie it together [image, seq] [word]\r\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n\r\n    # summarize model\r\n    print(model.summary())\r\n    plot_model(model, to_file='model.png', show_shapes=True)\r\n\r\n    return model\r\n8. Training the model\r\n\r\nTo train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability.\r\n# train our model\r\nprint('Dataset: ', len(train_imgs))\r\nprint('Descriptions: train=', len(train_descriptions))\r\nprint('Photos: train=', len(train_features))\r\nprint('Vocabulary Size:', vocab_size)\r\nprint('Description Length: ', max_length)\r\n\r\nmodel = define_model(vocab_size, max_length)\r\nepochs = 10\r\nsteps = len(train_descriptions)\r\n# making a directory models to save our models\r\nos.mkdir(\"models\")\r\nfor i in range(epochs):\r\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\r\n    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\r\n    model.save(\"models\/model_\" + str(i) + \".h5\")\r\n9. Testing the model\r\n\r\nThe model has been trained, now, we will make a separate file testing_caption_generator.py which will load the model and generate predictions. The predictions contain the max length of index values so we will use the same tokenizer.p pickle file to get the words from their index values.\r\n\r\nCode:\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport argparse\r\n\r\n\r\nap = argparse.ArgumentParser()\r\nap.add_argument('-i', '--image', required=True, help=\"Image Path\")\r\nargs = vars(ap.parse_args())\r\nimg_path = args['image']\r\n\r\ndef extract_features(filename, model):\r\n        try:\r\n            image = Image.open(filename)\r\n\r\n        except:\r\n            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\r\n        image = image.resize((299,299))\r\n        image = np.array(image)\r\n        # for images that has 4 channels, we convert them into 3 channels\r\n        if image.shape[2] == 4: \r\n            image = image[..., :3]\r\n        image = np.expand_dims(image, axis=0)\r\n        image = image\/127.5\r\n        image = image - 1.0\r\n        feature = model.predict(image)\r\n        return feature\r\n\r\ndef word_for_id(integer, tokenizer):\r\nfor word, index in tokenizer.word_index.items():\r\n     if index == integer:\r\n         return word\r\nreturn None\r\n\r\n\r\ndef generate_desc(model, tokenizer, photo, max_length):\r\n    in_text = 'start'\r\n    for i in range(max_length):\r\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\r\n        sequence = pad_sequences([sequence], maxlen=max_length)\r\n        pred = model.predict([photo,sequence], verbose=0)\r\n        pred = np.argmax(pred)\r\n        word = word_for_id(pred, tokenizer)\r\n        if word is None:\r\n            break\r\n        in_text += ' ' + word\r\n        if word == 'end':\r\n            break\r\n    return in_text\r\n\r\n\r\n#path = 'Flicker8k_Dataset\/111537222_07e56d5a30.jpg'\r\nmax_length = 32\r\ntokenizer = load(open(\"tokenizer.p\",\"rb\"))\r\nmodel = load_model('models\/model_9.h5')\r\nxception_model = Xception(include_top=False, pooling=\"avg\")\r\n\r\nphoto = extract_features(img_path, xception_model)\r\nimg = Image.open(img_path)\r\n\r\ndescription = generate_desc(model, tokenizer, photo, max_length)\r\nprint(\"\\n\\n\")\r\nprint(description)\r\nplt.imshow(img)\r\n\r\n\r\nResults:\r\n\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nIn this advanced Python project, we have implemented a CNN-RNN model by building an image caption generator. Some key points to note are that our model depends on the data, so, it cannot predict the words that are out of its vocabulary. We used a small dataset consisting of 8000 images. For production-level models, we need to train on datasets larger than 100,000 images which can produce better accuracy models.\r\nRock the Python interview round\r\nPractise 150+ Python Interview Questions\r\nHope you enjoyed making this Python based project with us. You can ask your doubts in the comment section below.","keywords":"Advanced python project, Image Caption Generator, python based project, Python data science project, Python project, ","name":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM","thumbnailUrl":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM.jpg","wordCount":"4146","timeRequired":"1105","mainEntity":{"@type":"WebPage","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/"},"author":{"@type":"Person","name":"DataFlair Team","description":"","image":{"@type":"ImageObject","url":"https:\/\/secure.gravatar.com\/avatar\/e7a40b33d21731d2b153f9fc0dc3497e?s=96&d=mm&r=g","height":96,"width":96}},"publisher":{"@id":"https:\/\/data-flair.training\/blogs#Organization"},"image":[{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/working-of-Deep-CNN-Python-project.png","width":"876","height":"318"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/LSTM-Cell-Structure-project-in-python.png","width":"1300","height":"853"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/Model-of-Image-Caption-Generator-python-project.png","width":"1648","height":"868"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/structure-python-data-science-project.png","width":"761","height":"429"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/jupyter-lab-advanced-python-project-1.jpg","width":"846","height":"517"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/token-file-project-in-python.png","width":"903","height":"440"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/descriptions-python-project-1.png","width":"1148","height":"639"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/save-descriptions-python-project.png","width":"690","height":"291"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/extracting_features.png","width":"805","height":"641"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/model-python-machine-learning-project.png","width":"829","height":"737"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-standing-on-rock.png","width":"1366","height":"522"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-girls-playing.png","width":"1366","height":"532"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-on-kayak.png","width":"1366","height":"531"},[{"@type":"ImageObject","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-1280x720.jpg","width":"1280","height":"720"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-640x480.jpg","width":"640","height":"480"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-300x300.jpg","width":"300","height":"300"}]],"isPartOf":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage"}}]},

{"@context":"https:\/\/schema.org","@graph":[{"@type":"EducationalOrganization","@id":"https:\/\/data-flair.training\/blogs#Organization","name":"DataFlair","url":"https:\/\/data-flair.training\/blogs","sameAs":[["https:\/\/www.facebook.com\/DataFlairWS\/"],["https:\/\/twitter.com\/DataFlairWS"],["https:\/\/www.instagram.com\/dataflair\/"],["https:\/\/www.youtube.com\/user\/DataFlairWS"],["https:\/\/www.linkedin.com\/company\/dataflair-web-services-pvt-ltd"],["https:\/\/in.pinterest.com\/DataFlair_Big_Data_Science\/"]],"logo":{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2016\/11\/DataFlair_Logo_.png","width":"150","height":"68"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","telephone":"+91-8451097879","url":"https:\/\/data-flair.training\/contact-us\/"}},{"@type":"WebSite","@id":"https:\/\/data-flair.training\/blogs#website","headline":"DataFlair","name":"DataFlair","description":"Learn Today. Lead Tomorrow.","url":"https:\/\/data-flair.training\/blogs","potentialAction":{"@type":"SearchAction","target":"https:\/\/data-flair.training\/blogs\/?s={search_term_string}","query-input":"required name=search_term_string"},"publisher":{"@id":"https:\/\/data-flair.training\/blogs#Organization"}},{"@context":"https:\/\/schema.org","@type":"WebPage","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage","name":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM","url":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","description":"Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN & a type of RNN (LSTM) together.","image":[{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/working-of-Deep-CNN-Python-project.png","width":"876","height":"318"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/LSTM-Cell-Structure-project-in-python.png","width":"1300","height":"853"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/Model-of-Image-Caption-Generator-python-project.png","width":"1648","height":"868"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/structure-python-data-science-project.png","width":"761","height":"429"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/jupyter-lab-advanced-python-project-1.jpg","width":"846","height":"517"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/token-file-project-in-python.png","width":"903","height":"440"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/descriptions-python-project-1.png","width":"1148","height":"639"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/save-descriptions-python-project.png","width":"690","height":"291"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/extracting_features.png","width":"805","height":"641"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/model-python-machine-learning-project.png","width":"829","height":"737"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-standing-on-rock.png","width":"1366","height":"522"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-girls-playing.png","width":"1366","height":"532"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-on-kayak.png","width":"1366","height":"531"},[{"@type":"ImageObject","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-1280x720.jpg","width":"1280","height":"720"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-640x480.jpg","width":"640","height":"480"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-300x300.jpg","width":"300","height":"300"}]],"primaryImageOfPage":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage"},"isPartOf":{"@id":"https:\/\/data-flair.training\/blogs#website"},"breadcrumb":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#breadcrumb"}},{"@type":"BreadcrumbList","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/data-flair.training\/blogs","name":"DataFlair"}},{"@type":"ListItem","position":2,"item":{"@id":"https:\/\/data-flair.training\/blogs\/category\/python\/","name":"Python Tutorials"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","name":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM"}}]},{"@type":"Article","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#article","url":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","mainEntityOfPage":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage","headline":"Python based Project &#8211; Learn to Build Image Caption Generator with CNN &amp; LSTM","description":"Python based project on image caption generator - Learn to build a working model of image caption generator by implementing CNN & a type of RNN (LSTM) together.","articleBody":"Project based on Python - Image Caption Generator\u00a0\r\n\r\nYou saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? Computer vision researchers worked on this a lot and they considered it impossible until now! With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image.\r\n\r\nThis is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together.\r\n\r\nBelow are some of the Python Data Science projects on which you can work later on:\r\n\r\n \tFake News Detection Python Project\r\n \tParkinson\u2019s Disease Detection Python Project\r\n \tColor Detection Python Project\r\n \tSpeech Emotion Recognition Python Project\r\n \tBreast Cancer Classification Python Project\r\n \tAge and Gender Detection Python Project\r\n \tHandwritten Digit Recognition Python Project\r\n \tChatbot Python Project\r\n \tDriver Drowsiness Detection Python Project\r\n \tTraffic Signs Recognition Python Project\r\n \tImage Caption Generator Python Project\r\n\r\nNow, let's quickly start the Python based project by defining the image caption generator.\r\nWhat is Image Caption Generator?\r\nImage caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English.\r\nImage Caption Generator with CNN - About the Python based Project\r\nThe objective of our project is to learn the concepts of a CNN and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM.\r\n\r\nIn this Python project, we will be implementing the caption generator using CNN (Convolutional Neural Networks) and LSTM (Long short term memory). The image features will be extracted from Xception which is a CNN model trained on the imagenet dataset and then we feed the features into the LSTM model which will be responsible for generating the image captions.\r\nThe Dataset of Python based Project\r\nFor the image caption generator, we will be using the Flickr_8K dataset. There are also other big datasets like Flickr_30K and MSCOCO dataset but it can take weeks just to train the network so we will be using a small Flickr8k dataset. The advantage of a huge dataset is that we can build better models.\r\n\r\nThanks to Jason Brownlee for providing a direct link to download the dataset (Size: 1GB).\r\n\r\n \tFlicker8k_Dataset\u00a0\r\n \tFlickr_8k_text\u00a0\r\n\r\nThe Flickr_8k_text folder contains file Flickr8k.token which is the main file of our dataset that contains image name and their respective captions separated by newline(\u201c\\n\u201d).\r\nPre-requisites\r\nThis project requires good knowledge of Deep learning, Python, working on Jupyter notebooks, Keras library, Numpy, and Natural language processing.\r\n\r\nMake sure you have installed all the following necessary libraries:\r\n\r\n \tpip install tensorflow\r\n \tkeras\r\n \tpillow\r\n \tnumpy\r\n \ttqdm\r\n \tjupyterlab\r\n\r\nImage Caption Generator - Python based Project\r\nWhat is CNN?\r\nConvolutional Neural networks are specialized deep neural networks which can process the data that has input shape like a 2D matrix. Images are easily represented as a 2D matrix and CNN is very useful in working with images.\r\n\r\nCNN is basically used for image classifications and identifying if an image is a bird, a plane or Superman, etc.\r\n\r\nIt scans images from left to right and top to bottom to pull out important features from the image and combines the feature to classify images. It can handle the images that have been translated, rotated, scaled and changes in perspective.\r\nPractise the important Python topics\r\nCheck out the 240+ Python Tutorials\r\n\r\nWhat is LSTM?\r\nLSTM stands for Long short term memory, they are a type of RNN (recurrent neural network) which is well suited for sequence prediction problems. Based on the previous text, we can predict what the next word will be. It has proven itself effective from the traditional RNN by overcoming the limitations of RNN which had short term memory. LSTM can carry out relevant information throughout the processing of inputs and with a forget gate, it discards non-relevant information.\r\n\r\nThis is what an LSTM cell looks like -\r\n\r\n\r\nImage Caption Generator Model\r\nSo, to make our image caption generator model, we will be merging these architectures. It is also called a CNN-RNN model.\r\n\r\n \tCNN is used for extracting features from the image. We will use the pre-trained model Xception.\r\n \tLSTM will use the information from CNN to help generate a description of the image.\r\n\r\n\r\n\r\nProject File Structure\r\nDownloaded from dataset:\r\n\r\n \tFlicker8k_Dataset - Dataset folder which contains 8091 images.\r\n \tFlickr_8k_text - Dataset folder which contains text files and captions of images.\r\n\r\nThe below files will be created by us while making the project.\r\n\r\n \tModels - It will contain our trained models.\r\n \tDescriptions.txt - This text file contains all image names and their captions after preprocessing.\r\n \tFeatures.p - Pickle object that contains an image and their feature vector extracted from the Xception pre-trained CNN model.\r\n \tTokenizer.p - Contains tokens mapped with an index value.\r\n \tModel.png - Visual representation of dimensions of our project.\r\n \tTesting_caption_generator.py - Python file for generating a caption of any image.\r\n \tTraining_caption_generator.ipynb - Jupyter notebook in which we train and build our image caption generator.\r\n\r\nYou can download all the files from the link:\r\n\r\nImage Caption Generator - Python Project Files\r\n\r\n\r\nWant to become a Python expert?\r\nEnroll for the Certified Python Training Course\r\n\r\nBuilding the Python based Project\r\nLet\u2019s start by initializing the jupyter notebook server by typing jupyter lab in the console of your project folder. It will open up the interactive Python notebook where you can run your code. Create a Python3 notebook and name it training_caption_generator.ipynb\r\n\r\n1. First, we import all the necessary packages\r\nimport string\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport os\r\nfrom pickle import dump, load\r\nimport numpy as np\r\n\r\nfrom keras.applications.xception import Xception, preprocess_input\r\nfrom keras.preprocessing.image import load_img, img_to_array\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.utils import to_categorical\r\nfrom keras.layers.merge import add\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout\r\n\r\n# small library for seeing the progress of loops.\r\nfrom tqdm import tqdm_notebook as tqdm\r\ntqdm().pandas()\r\n2. Getting and performing data cleaning\r\n\r\nThe main text file which contains all image captions is Flickr8k.token in our Flickr_8k_text folder.\r\n\r\nHave a look at the file -\r\n\r\nThe format of our file is image and caption separated by a new line (\u201c\\n\u201d).\r\n\r\nEach image has 5 captions and we can see that #(0 to 5)number is assigned for each caption.\r\n\r\nWe will define 5 functions:\r\n\r\n \tload_doc( filename ) - For loading the document file and reading the contents inside the file into a string.\r\n \tall_img_captions( filename ) - This function will create a descriptions dictionary that maps images with a list of 5 captions. The descriptions dictionary will look something like this:\r\n\r\n\r\n\r\n\r\n \tcleaning_text( descriptions) - This function takes all descriptions and performs data cleaning. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers.\r\nSo, a caption like \u201cA man riding on a three-wheeled wheelchair\u201d will be transformed into \u201cman riding on three wheeled wheelchair\u201d\r\n \ttext_vocabulary( descriptions ) - This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.\r\n \tsave_descriptions( descriptions, filename ) - This function will create a list of all the descriptions that have been preprocessed and store them into a file. We will create a descriptions.txt file to store all the captions. It will look something like this:\r\n\r\n\r\nCode :\r\n# Loading a text file into memory\r\ndef load_doc(filename):\r\n    # Opening the file as read only\r\n    file = open(filename, 'r')\r\n    text = file.read()\r\n    file.close()\r\n    return text\r\n\r\n# get all imgs with their captions\r\ndef all_img_captions(filename):\r\n    file = load_doc(filename)\r\n    captions = file.split('\\n')\r\n    descriptions ={}\r\n    for caption in captions[:-1]:\r\n        img, caption = caption.split('\\t')\r\n        if img[:-2] not in descriptions:\r\n            descriptions[img[:-2]] = \r\n        else:\r\n            descriptions[img[:-2]].append(caption)\r\n    return descriptions\r\n\r\n#Data cleaning- lower casing, removing puntuations and words containing numbers\r\ndef cleaning_text(captions):\r\n    table = str.maketrans('','',string.punctuation)\r\n    for img,caps in captions.items():\r\n        for i,img_caption in enumerate(caps):\r\n\r\n            img_caption.replace(\"-\",\" \")\r\n            desc = img_caption.split()\r\n\r\n            #converts to lowercase\r\n            desc = [word.lower() for word in desc]\r\n            #remove punctuation from each token\r\n            desc = [word.translate(table) for word in desc]\r\n            #remove hanging 's and a \r\n            desc = [word for word in desc if(len(word)&gt;1)]\r\n            #remove tokens with numbers in them\r\n            desc = [word for word in desc if(word.isalpha())]\r\n            #convert back to string\r\n\r\n            img_caption = ' '.join(desc)\r\n            captions[img][i]= img_caption\r\n    return captions\r\n\r\ndef text_vocabulary(descriptions):\r\n    # build vocabulary of all unique words\r\n    vocab = set()\r\n\r\n    for key in descriptions.keys():\r\n        [vocab.update(d.split()) for d in descriptions[key]]\r\n\r\n    return vocab\r\n\r\n#All descriptions in one file \r\ndef save_descriptions(descriptions, filename):\r\n    lines = list()\r\n    for key, desc_list in descriptions.items():\r\n        for desc in desc_list:\r\n            lines.append(key + '\\t' + desc )\r\n    data = \"\\n\".join(lines)\r\n    file = open(filename,\"w\")\r\n    file.write(data)\r\n    file.close()\r\n\r\n\r\n# Set these path according to project folder in you system\r\ndataset_text = \"D:\\dataflair projects\\Project - Image Caption Generator\\Flickr_8k_text\"\r\ndataset_images = \"D:\\dataflair projects\\Project - Image Caption Generator\\Flicker8k_Dataset\"\r\n\r\n#we prepare our text data\r\nfilename = dataset_text + \"\/\" + \"Flickr8k.token.txt\"\r\n#loading the file that contains all data\r\n#mapping them into descriptions dictionary img to 5 captions\r\ndescriptions = all_img_captions(filename)\r\nprint(\"Length of descriptions =\" ,len(descriptions))\r\n\r\n#cleaning the descriptions\r\nclean_descriptions = cleaning_text(descriptions)\r\n\r\n#building vocabulary \r\nvocabulary = text_vocabulary(clean_descriptions)\r\nprint(\"Length of vocabulary = \", len(vocabulary))\r\n\r\n#saving each description to file \r\nsave_descriptions(clean_descriptions, \"descriptions.txt\")\r\n3. Extracting the feature vector from all images\u00a0\r\n\r\nThis technique is also called transfer learning, we don\u2019t have to do everything on our own, we use the pre-trained model that have been already trained on large datasets and extract the features from these models and use them for our tasks. We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. We can directly import this model from the keras.applications . Make sure you are connected to the internet as the weights get automatically downloaded. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.\r\n\r\nmodel = Xception( include_top=False, pooling='avg' )\r\n\r\nThe function extract_features() will extract features for all images and we will map image names with their respective feature array. Then we will dump the features dictionary into a \u201cfeatures.p\u201d pickle file.\r\n\r\nCode:\r\ndef extract_features(directory):\r\n        model = Xception( include_top=False, pooling='avg' )\r\n        features = {}\r\n        for img in tqdm(os.listdir(directory)):\r\n            filename = directory + \"\/\" + img\r\n            image = Image.open(filename)\r\n            image = image.resize((299,299))\r\n            image = np.expand_dims(image, axis=0)\r\n            #image = preprocess_input(image)\r\n            image = image\/127.5\r\n            image = image - 1.0\r\n\r\n            feature = model.predict(image)\r\n            features[img] = feature\r\n        return features\r\n\r\n#2048 feature vector\r\nfeatures = extract_features(dataset_images)\r\ndump(features, open(\"features.p\",\"wb\"))\r\n\r\nThis process can take a lot of time depending on your system. I am using an Nvidia 1050 GPU for training purpose so it took me around 7 minutes for performing this task. However, if you are using CPU then this process might take 1-2 hours. You can comment out the code and directly load the features from our pickle file.\r\nfeatures = load(open(\"features.p\",\"rb\"))\r\n4. Loading dataset for Training the model\r\n\r\nIn our Flickr_8k_test folder, we have Flickr_8k.trainImages.txt file that contains a list of 6000 image names that we will use for training.\r\n\r\nFor loading the training dataset, we need more functions:\r\n\r\n \tload_photos( filename ) - This will load the text file in a string and will return the list of image names.\r\n \tload_clean_descriptions( filename, photos ) - This function will create a dictionary that contains captions for each photo from the list of photos. We also append the &lt;start&gt; and &lt;end&gt; identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.\r\n \tload_features(photos) - This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model.\r\n\r\nCode :\r\n#load the data \r\ndef load_photos(filename):\r\n    file = load_doc(filename)\r\n    photos = file.split(\"\\n\")[:-1]\r\n    return photos\r\n\r\n\r\ndef load_clean_descriptions(filename, photos): \r\n    #loading clean_descriptions\r\n    file = load_doc(filename)\r\n    descriptions = {}\r\n    for line in file.split(\"\\n\"):\r\n\r\n        words = line.split()\r\n        if len(words)&lt;1 :\r\n            continue\r\n\r\n        image, image_caption = words[0], words[1:]\r\n\r\n        if image in photos:\r\n            if image not in descriptions:\r\n                descriptions[image] = []\r\n            desc = '&lt;start&gt; ' + \" \".join(image_caption) + ' &lt;end&gt;'\r\n            descriptions[image].append(desc)\r\n\r\n    return descriptions\r\n\r\n\r\ndef load_features(photos):\r\n    #loading all features\r\n    all_features = load(open(\"features.p\",\"rb\"))\r\n    #selecting only needed features\r\n    features = {k:all_features[k] for k in photos}\r\n    return features\r\n\r\n\r\nfilename = dataset_text + \"\/\" + \"Flickr_8k.trainImages.txt\"\r\n\r\n#train = loading_data(filename)\r\ntrain_imgs = load_photos(filename)\r\ntrain_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\r\ntrain_features = load_features(train_imgs)\r\n5. Tokenizing the vocabulary\u00a0\r\n\r\nComputers don\u2019t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a \u201ctokenizer.p\u201d pickle file.\r\n\r\nCode:\r\n#converting dictionary to clean list of descriptions\r\ndef dict_to_list(descriptions):\r\n    all_desc = []\r\n    for key in descriptions.keys():\r\n        [all_desc.append(d) for d in descriptions[key]]\r\n    return all_desc\r\n\r\n#creating tokenizer class \r\n#this will vectorise text corpus\r\n#each integer will represent token in dictionary\r\n\r\nfrom keras.preprocessing.text import Tokenizer\r\n\r\ndef create_tokenizer(descriptions):\r\n    desc_list = dict_to_list(descriptions)\r\n    tokenizer = Tokenizer()\r\n    tokenizer.fit_on_texts(desc_list)\r\n    return tokenizer\r\n\r\n# give each word an index, and store that into tokenizer.p pickle file\r\ntokenizer = create_tokenizer(train_descriptions)\r\ndump(tokenizer, open('tokenizer.p', 'wb'))\r\nvocab_size = len(tokenizer.word_index) + 1\r\nvocab_size\r\nOur vocabulary contains 7577 words.\r\n\r\nWe calculate the maximum length of the descriptions. This is important for deciding the model structure parameters. Max_length of description is 32.\r\n#calculate maximum length of descriptions\r\ndef max_length(descriptions):\r\n    desc_list = dict_to_list(descriptions)\r\n    return max(len(d.split()) for d in desc_list)\r\n    \r\nmax_length = max_length(descriptions)\r\nmax_length\r\n6. Create Data generator\r\n\r\nLet us first see how the input and output of our model will look like. To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\r\n\r\nThe generator will yield the input and output sequence.\r\n\r\nFor example:\r\n\r\nThe input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict.\r\n\r\n\r\n\r\nx1(feature vector)\r\nx2(Text sequence)\r\ny(word to predict)\r\n\r\n\r\nfeature\r\nstart,\r\ntwo\r\n\r\n\r\nfeature\r\nstart, two\r\ndogs\r\n\r\n\r\nfeature\r\nstart, two, dogs\r\ndrink\r\n\r\n\r\nfeature\r\nstart, two, dogs, drink\r\nwater\r\n\r\n\r\nfeature\r\nstart, two, dogs, drink, water\r\nend\r\n\r\n\r\n\r\n#create input-output sequence pairs from the image description.\r\n\r\n#data generator, used by model.fit_generator()\r\ndef data_generator(descriptions, features, tokenizer, max_length):\r\n    while 1:\r\n        for key, description_list in descriptions.items():\r\n            #retrieve photo features\r\n            feature = features[key][0]\r\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\r\n            yield [[input_image, input_sequence], output_word]\r\n\r\ndef create_sequences(tokenizer, max_length, desc_list, feature):\r\n    X1, X2, y = list(), list(), list()\r\n    # walk through each description for the image\r\n    for desc in desc_list:\r\n        # encode the sequence\r\n        seq = tokenizer.texts_to_sequences([desc])[0]\r\n        # split one sequence into multiple X,y pairs\r\n        for i in range(1, len(seq)):\r\n            # split into input and output pair\r\n            in_seq, out_seq = seq[:i], seq[i]\r\n            # pad input sequence\r\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n            # encode output sequence\r\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n            # store\r\n            X1.append(feature)\r\n            X2.append(in_seq)\r\n            y.append(out_seq)\r\n    return np.array(X1), np.array(X2), np.array(y)\r\n\r\n#You can check the shape of the input and output for your model\r\n[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\r\na.shape, b.shape, c.shape\r\n#((47, 2048), (47, 32), (47, 7577))\r\n7. Defining the CNN-RNN model\r\n\r\nTo define the structure of the model, we will be using the Keras Model from Functional API. It will consist of three major parts:\r\n\r\n \tFeature Extractor - The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.\r\n \tSequence Processor - An embedding layer will handle the textual input, followed by the LSTM layer.\r\n \tDecoder - By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size.\r\n\r\nVisual representation of the final model is given below -\r\n\r\n\r\nfrom keras.utils import plot_model\r\n\r\n# define the captioning model\r\ndef define_model(vocab_size, max_length):\r\n\r\n    # features from the CNN model squeezed from 2048 to 256 nodes\r\n    inputs1 = Input(shape=(2048,))\r\n    fe1 = Dropout(0.5)(inputs1)\r\n    fe2 = Dense(256, activation='relu')(fe1)\r\n\r\n    # LSTM sequence model\r\n    inputs2 = Input(shape=(max_length,))\r\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\r\n    se2 = Dropout(0.5)(se1)\r\n    se3 = LSTM(256)(se2)\r\n\r\n    # Merging both models\r\n    decoder1 = add([fe2, se3])\r\n    decoder2 = Dense(256, activation='relu')(decoder1)\r\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n\r\n    # tie it together [image, seq] [word]\r\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n\r\n    # summarize model\r\n    print(model.summary())\r\n    plot_model(model, to_file='model.png', show_shapes=True)\r\n\r\n    return model\r\n8. Training the model\r\n\r\nTo train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability.\r\n# train our model\r\nprint('Dataset: ', len(train_imgs))\r\nprint('Descriptions: train=', len(train_descriptions))\r\nprint('Photos: train=', len(train_features))\r\nprint('Vocabulary Size:', vocab_size)\r\nprint('Description Length: ', max_length)\r\n\r\nmodel = define_model(vocab_size, max_length)\r\nepochs = 10\r\nsteps = len(train_descriptions)\r\n# making a directory models to save our models\r\nos.mkdir(\"models\")\r\nfor i in range(epochs):\r\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\r\n    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\r\n    model.save(\"models\/model_\" + str(i) + \".h5\")\r\n9. Testing the model\r\n\r\nThe model has been trained, now, we will make a separate file testing_caption_generator.py which will load the model and generate predictions. The predictions contain the max length of index values so we will use the same tokenizer.p pickle file to get the words from their index values.\r\n\r\nCode:\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport argparse\r\n\r\n\r\nap = argparse.ArgumentParser()\r\nap.add_argument('-i', '--image', required=True, help=\"Image Path\")\r\nargs = vars(ap.parse_args())\r\nimg_path = args['image']\r\n\r\ndef extract_features(filename, model):\r\n        try:\r\n            image = Image.open(filename)\r\n\r\n        except:\r\n            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\r\n        image = image.resize((299,299))\r\n        image = np.array(image)\r\n        # for images that has 4 channels, we convert them into 3 channels\r\n        if image.shape[2] == 4: \r\n            image = image[..., :3]\r\n        image = np.expand_dims(image, axis=0)\r\n        image = image\/127.5\r\n        image = image - 1.0\r\n        feature = model.predict(image)\r\n        return feature\r\n\r\ndef word_for_id(integer, tokenizer):\r\nfor word, index in tokenizer.word_index.items():\r\n     if index == integer:\r\n         return word\r\nreturn None\r\n\r\n\r\ndef generate_desc(model, tokenizer, photo, max_length):\r\n    in_text = 'start'\r\n    for i in range(max_length):\r\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\r\n        sequence = pad_sequences([sequence], maxlen=max_length)\r\n        pred = model.predict([photo,sequence], verbose=0)\r\n        pred = np.argmax(pred)\r\n        word = word_for_id(pred, tokenizer)\r\n        if word is None:\r\n            break\r\n        in_text += ' ' + word\r\n        if word == 'end':\r\n            break\r\n    return in_text\r\n\r\n\r\n#path = 'Flicker8k_Dataset\/111537222_07e56d5a30.jpg'\r\nmax_length = 32\r\ntokenizer = load(open(\"tokenizer.p\",\"rb\"))\r\nmodel = load_model('models\/model_9.h5')\r\nxception_model = Xception(include_top=False, pooling=\"avg\")\r\n\r\nphoto = extract_features(img_path, xception_model)\r\nimg = Image.open(img_path)\r\n\r\ndescription = generate_desc(model, tokenizer, photo, max_length)\r\nprint(\"\\n\\n\")\r\nprint(description)\r\nplt.imshow(img)\r\n\r\n\r\nResults:\r\n\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nIn this advanced Python project, we have implemented a CNN-RNN model by building an image caption generator. Some key points to note are that our model depends on the data, so, it cannot predict the words that are out of its vocabulary. We used a small dataset consisting of 8000 images. For production-level models, we need to train on datasets larger than 100,000 images which can produce better accuracy models.\r\nRock the Python interview round\r\nPractise 150+ Python Interview Questions\r\nHope you enjoyed making this Python based project with us. You can ask your doubts in the comment section below.","keywords":"Advanced python project, Image Caption Generator, python based project, Python data science project, Python project, ","datePublished":"2019-11-14T12:23:48Z","dateModified":"2020-01-07T10:15:02Z","author":{"@type":"Person","name":"DataFlair Team","description":"","image":{"@type":"ImageObject","url":"https:\/\/secure.gravatar.com\/avatar\/e7a40b33d21731d2b153f9fc0dc3497e?s=96&d=mm&r=g","height":96,"width":96}},"publisher":{"@id":"https:\/\/data-flair.training\/blogs#Organization"},"image":[{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/working-of-Deep-CNN-Python-project.png","width":"876","height":"318"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/LSTM-Cell-Structure-project-in-python.png","width":"1300","height":"853"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/Model-of-Image-Caption-Generator-python-project.png","width":"1648","height":"868"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/structure-python-data-science-project.png","width":"761","height":"429"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/jupyter-lab-advanced-python-project-1.jpg","width":"846","height":"517"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/token-file-project-in-python.png","width":"903","height":"440"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/descriptions-python-project-1.png","width":"1148","height":"639"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/save-descriptions-python-project.png","width":"690","height":"291"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/extracting_features.png","width":"805","height":"641"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/model-python-machine-learning-project.png","width":"829","height":"737"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-standing-on-rock.png","width":"1366","height":"522"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-girls-playing.png","width":"1366","height":"532"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/image-caption-generator-man-on-kayak.png","width":"1366","height":"531"},[{"@type":"ImageObject","@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#primaryimage","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-1280x720.jpg","width":"1280","height":"720"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-640x480.jpg","width":"640","height":"480"},{"@type":"ImageObject","url":"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/11\/python-project-image-caption-generator-with-CNN-and-LSTM-300x300.jpg","width":"300","height":"300"}]],"isPartOf":{"@id":"https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/#webpage"}}]}]</script> <link rel="amphtml" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/amp/"><link rel="icon" href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2016/11/cropped-DataFlair_Logo_Final_220x100_Transp-32x32.png" sizes="32x32"><link rel="icon" href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2016/11/cropped-DataFlair_Logo_Final_220x100_Transp-192x192.png" sizes="192x192"><link rel="apple-touch-icon-precomposed" href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2016/11/cropped-DataFlair_Logo_Final_220x100_Transp-180x180.png"><meta name="msapplication-TileImage" content="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2016/11/cropped-DataFlair_Logo_Final_220x100_Transp-270x270.png"><style id="wfc-style-body" data-origin="server">/* Setting : Default website font */ 
body {
}</style><style id="grids-css">.post-list .grid-item {float: left; }
                .cols-1 .grid-item { width: 100%; }
                .cols-2 .grid-item { width: 50%; }
                .cols-3 .grid-item { width: 33.3%; }
                .cols-4 .grid-item { width: 25%; }
                @media only screen and (max-width: 719px) {
                      #grid-wrapper .grid-item{
                        width: 100%;
                      }
                }</style><style>.ai-viewport-3                { display: none !important;}
.ai-viewport-2                { display: none !important;}
.ai-viewport-1                { display: inherit !important;}
.ai-viewport-0                { display: none !important;}
@media (min-width: 768px) and (max-width: 979px) {
.ai-viewport-1                { display: none !important;}
.ai-viewport-2                { display: inherit !important;}
}
@media (max-width: 767px) {
.ai-viewport-1                { display: none !important;}
.ai-viewport-3                { display: inherit !important;}
}</style> <script async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(1).txt"></script> <script>(adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-9834688387117374",
          enable_page_level_ads: true
     });</script>  <script async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/js"></script> <script data-cfasync="false">(function(w, d) { var s = d.createElement('script'); s.src = '//cdn.adpushup.com/41212/adpushup.js'; s.type = 'text/javascript'; s.async = true; (d.getElementsByTagName('head')[0] || d.getElementsByTagName('body')[0]).appendChild(s); })(window, document);</script><script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/adpushup.js.download" type="text/javascript" async=""></script> <script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59844492-2');</script> <meta name="p:domain_verify" content="e08b98e1059afe95078ceeec78a98050"> <script async="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(1).txt"></script> <script>(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9834688387117374",
    enable_page_level_ads: true
  });</script> <link rel="preload" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(2).txt" as="script"><script type="text/javascript" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(2).txt"></script><link rel="preload" href="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(3).txt" as="script"><script type="text/javascript" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/f(3).txt"></script></head><body class="post-template-default single single-post postid-72771 single-format-standard wp-custom-logo col-3cm full-width topbar-enabled mobile-sidebar-hide header-desktop-sticky header-mobile-sticky chrome hueman-pro"><div id="wrapper"><header id="header" class="specific-mobile-menu-on one-mobile-menu mobile_menu header-ads-desktop  topbar-transparent no-header-img"><nav class="nav-container group mobile-menu mobile-sticky " id="nav-mobile" data-menu-id="header-1"><div class="mobile-title-logo-in-header"><p class="site-title"><a class="custom-logo-link" href="https://data-flair.training/blogs/" rel="home" title="DataFlair | Home page"><img src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/DataFlair_Logo_99x45.png" alt="DataFlair"></a></p></div><div class="ham__navbar-toggler-two collapsed" title="Menu" aria-expanded="false"><div class="ham__navbar-span-wrapper">
<span class="line line-1"></span>
<span class="line line-2"></span>
<span class="line line-3"></span></div></div><div class="nav-text"></div><div class="nav-wrap container"><ul class="nav container-inner group mobile-search"><li><form method="get" class="searchform themeform" action="https://data-flair.training/blogs/"><div>
<input type="text" class="search" name="s" onblur="if(this.value==&#39;&#39;)this.value=&#39;To search type and hit enter&#39;;" onfocus="if(this.value==&#39;To search type and hit enter&#39;)this.value;" value="To search type and hit enter"></div></form></li></ul><ul id="menu-mobile-nav" class="nav container-inner group"><li id="menu-item-75198" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-75198"><a href="https://data-flair.training/blogs/" data-ps2id-api="true">Blogs</a></li><li id="menu-item-75201" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75201"><a href="https://data-flair.training/blogs/category/big-data/" data-ps2id-api="true">Big Data Tutorials</a></li><li id="menu-item-75203" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75203"><a href="https://data-flair.training/blogs/category/hadoop/" data-ps2id-api="true">Hadoop Tutorials</a></li><li id="menu-item-75205" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75205"><a href="https://data-flair.training/blogs/category/spark/" data-ps2id-api="true">Spark Tutorials</a></li><li id="menu-item-75202" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75202"><a href="https://data-flair.training/blogs/category/data-science/" data-ps2id-api="true">Data Science Tutorials</a></li><li id="menu-item-75199" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-75199"><a href="https://data-flair.training/blogs/category/python/" data-ps2id-api="true">Python Tutorials</a></li><li id="menu-item-75200" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75200"><a href="https://data-flair.training/blogs/category/r/" data-ps2id-api="true">R Tutorials</a></li><li id="menu-item-75204" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75204"><a href="https://data-flair.training/blogs/category/machine-learning/" data-ps2id-api="true">Machine Learning Tutorials</a></li></ul></div></nav><nav class="nav-container group desktop-menu desktop-sticky " id="nav-topbar" data-menu-id="header-2"><div class="nav-text"></div><div class="topbar-toggle-down">
<i class="fas fa-angle-double-down" aria-hidden="true" data-toggle="down" title="Expand menu"></i>
<i class="fas fa-angle-double-up" aria-hidden="true" data-toggle="up" title="Collapse menu"></i></div><div class="nav-wrap container"><ul id="menu-quiz" class="nav container-inner group"><li id="menu-item-77345" class="dflogo menu-item menu-item-type-custom menu-item-object-custom menu-item-77345"><a href="https://data-flair.training/" data-ps2id-api="true"></a></li><li id="menu-item-44066" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-44066"><a href="https://data-flair.training/blogs/" data-ps2id-api="true">Blog Home</a></li><li id="menu-item-27572" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27572"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Courses</a><ul class="sub-menu"><li id="menu-item-27610" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-27610"><a href="https://data-flair.training/hadoop-spark-developer-course/" data-ps2id-api="true">Big Data Hadoop &amp; Spark Scala</a></li><li id="menu-item-66601" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-66601"><a href="https://data-flair.training/python-course/" data-ps2id-api="true">Python Course</a></li><li id="menu-item-27611" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-27611"><a href="https://data-flair.training/big-data-hadoop/" data-ps2id-api="true">Big Data &amp; Hadoop</a></li><li id="menu-item-69115" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-69115"><a href="https://data-flair.training/apache-kafka/" data-ps2id-api="true">Apache Kafka</a></li><li id="menu-item-27612" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-27612"><a href="https://data-flair.training/apache-spark-scala/" data-ps2id-api="true">Apache Spark &amp; Scala</a></li></ul></li><li id="menu-item-27594" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27594"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Data Science</a><ul class="sub-menu"><li id="menu-item-67266" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-67266"><a href="https://data-flair.training/blogs/data-science-tutorials-home/" data-ps2id-api="true">Data Science Tutorials</a></li><li id="menu-item-56249" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-56249"><a href="https://data-flair.training/blogs/python-tutorials-home/" data-ps2id-api="true">Python Tutorials</a><ul class="sub-menu"><li id="menu-item-62285" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-62285"><a href="https://data-flair.training/blogs/python-tutorials-home/" data-ps2id-api="true">Python Tutorials</a></li><li id="menu-item-40111" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40111"><a href="https://data-flair.training/blogs/tensorflow-tutorials-home/" data-ps2id-api="true">TensorFlow Tutorials</a></li><li id="menu-item-56250" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-56250"><a href="https://data-flair.training/blogs/pandas-tutorials-home/" data-ps2id-api="true">Pandas Tutorials</a></li><li id="menu-item-56245" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-56245"><a href="https://data-flair.training/blogs/django-tutorials-home/" data-ps2id-api="true">Django Tutorials</a></li></ul></li><li id="menu-item-40112" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40112"><a href="https://data-flair.training/blogs/iot-tutorials-home/" data-ps2id-api="true">IoT Tutorials</a></li><li id="menu-item-27620" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27620"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">BI Tutorials</a><ul class="sub-menu"><li id="menu-item-40121" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40121"><a href="https://data-flair.training/blogs/tableau-tutorials-home/" data-ps2id-api="true">Tableau Tutorials</a></li><li id="menu-item-40123" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40123"><a href="https://data-flair.training/blogs/power-bi-tutorials-home/" data-ps2id-api="true">Power BI Tutorials</a></li><li id="menu-item-40122" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40122"><a href="https://data-flair.training/blogs/qlikview-tutorials-home/" data-ps2id-api="true">QlikView Tutorials</a></li><li id="menu-item-44076" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-44076"><a href="https://data-flair.training/blogs/qlik-sense-tutorials-home/" data-ps2id-api="true">Qlik Sense Tutorials</a></li><li id="menu-item-56246" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-56246"><a href="https://data-flair.training/blogs/sap-hana-tutorials-home/" data-ps2id-api="true">SAP HANA Tutorials</a></li></ul></li><li id="menu-item-67583" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-67583"><a href="https://data-flair.training/blogs/r-tutorials-home/" data-ps2id-api="true">R Tutorials</a></li><li id="menu-item-40114" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40114"><a href="https://data-flair.training/blogs/sas-tutorials-home/" data-ps2id-api="true">SAS Tutorials</a></li><li id="menu-item-40116" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40116"><a href="https://data-flair.training/blogs/sas-stat-tutorials-home/" data-ps2id-api="true">SAS – STAT Tutorials</a></li><li id="menu-item-68437" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-68437"><a href="https://data-flair.training/blogs/machine-learning-tutorials-home/" data-ps2id-api="true">Machine Learning Tutorials</a></li><li id="menu-item-40118" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40118"><a href="https://data-flair.training/blogs/ai-tutorials-home/" data-ps2id-api="true">AI Tutorials</a></li><li id="menu-item-40119" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40119"><a href="https://data-flair.training/blogs/data-mining-tutorials-home/" data-ps2id-api="true">Data Mining Tutorials</a></li><li id="menu-item-40120" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40120"><a href="https://data-flair.training/blogs/sql-tutorials-home/" data-ps2id-api="true">SQL Tutorials</a></li></ul></li><li id="menu-item-27617" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27617"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Big Data</a><ul class="sub-menu"><li id="menu-item-67263" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-67263"><a href="https://data-flair.training/blogs/big-data-tutorials-home/" data-ps2id-api="true">Big Data Tutorials</a></li><li id="menu-item-27595" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-27595"><a href="https://data-flair.training/blogs/hadoop-tutorials-home/" data-ps2id-api="true">Hadoop Ecosystem Tutorials</a></li><li id="menu-item-40141" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40141"><a href="https://data-flair.training/blogs/spark-tutorials-home/" data-ps2id-api="true">Apache Spark Tutorials</a></li><li id="menu-item-40142" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40142"><a href="https://data-flair.training/blogs/spark-tutorials-home/" data-ps2id-api="true">PySpark Tutorials</a></li><li id="menu-item-40143" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40143"><a href="https://data-flair.training/blogs/flink-tutorials-home/" data-ps2id-api="true">Apache Flink Tutorials</a></li><li id="menu-item-40144" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40144"><a href="https://data-flair.training/blogs/kafka-tutorials-home/" data-ps2id-api="true">Apache Kafka Tutorials</a></li><li id="menu-item-27609" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27609"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">NoSQL Databases</a><ul class="sub-menu"><li id="menu-item-40099" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40099"><a href="https://data-flair.training/blogs/cassandra-tutorials-home/" data-ps2id-api="true">Cassandra Tutorials</a></li><li id="menu-item-40101" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40101"><a href="https://data-flair.training/blogs/mongodb-tutorials-home/" data-ps2id-api="true">MongoDB Tutorials</a></li></ul></li></ul></li><li id="menu-item-27582" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27582"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Categories</a><ul class="sub-menu"><li id="menu-item-77342" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-77342"><a href="https://data-flair.training/blogs/android-tutorials-home/" data-ps2id-api="true">Android Tutorials</a></li><li id="menu-item-40102" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40102"><a href="https://data-flair.training/blogs/blockchain-tutorials-home/" data-ps2id-api="true">Blockchain Tutorials</a></li><li id="menu-item-45536" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-45536"><a href="https://data-flair.training/blogs/cloud-computing-tutorials-home/" data-ps2id-api="true">Cloud Computing Tutorials</a></li><li id="menu-item-40103" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40103"><a href="https://data-flair.training/blogs/aws-tutorials-home/" data-ps2id-api="true">AWS Tutorials</a></li><li id="menu-item-40104" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40104"><a href="https://data-flair.training/blogs/salesforce-tutorials-home/" data-ps2id-api="true">Salesforce Tutorials</a></li><li id="menu-item-75349" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-75349"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Programming</a><ul class="sub-menu"><li id="menu-item-58606" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-58606"><a href="https://data-flair.training/blogs/c-tutorials-home/" data-ps2id-api="true">C Tutorials</a></li><li id="menu-item-40107" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40107"><a href="https://data-flair.training/blogs/scala-tutorials-home/" data-ps2id-api="true">Scala Tutorials</a></li><li id="menu-item-40108" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40108"><a href="https://data-flair.training/blogs/java-tutorials-home/" data-ps2id-api="true">Java Tutorials</a></li><li id="menu-item-40109" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-40109"><a href="https://data-flair.training/blogs/spring-tutorials-home/" data-ps2id-api="true">Spring Tutorials</a></li></ul></li><li id="menu-item-27578" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-27578"><a href="https://data-flair.training/blogs/category/linux/" data-ps2id-api="true">Linux Tutorials</a></li><li id="menu-item-66879" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-66879"><a href="https://data-flair.training/blogs/javascript-tutorials-home/" data-ps2id-api="true">JavaScript Tutorials</a></li><li id="menu-item-75350" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-75350"><a href="https://data-flair.training/blogs/category/angularjs/" data-ps2id-api="true">AngularJS Tutorials</a></li><li id="menu-item-51339" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-51339"><a href="https://data-flair.training/blogs/seo-tutorials-home/" data-ps2id-api="true">SEO Tutorials</a></li></ul></li><li id="menu-item-27573" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-27573"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">Interview Questions</a><ul class="sub-menu"><li id="menu-item-44825" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44825"><a href="https://data-flair.training/blogs/data-science-tutorials-home/#interview-questions" data-ps2id-api="true">Data Science Interview Questions</a></li><li id="menu-item-44932" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44932"><a href="https://data-flair.training/blogs/python-tutorials-home/#interview-questions" data-ps2id-api="true">Python Interview Questions</a></li><li id="menu-item-42706" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-42706"><a href="https://data-flair.training/blogs/hadoop-tutorials-home/#interview-questions" data-ps2id-api="true">Hadoop Interview Questions</a></li><li id="menu-item-44914" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44914"><a href="https://data-flair.training/blogs/spark-tutorials-home/#interview-questions" data-ps2id-api="true">Spark Interview Questions</a></li><li id="menu-item-44857" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44857"><a href="https://data-flair.training/blogs/r-tutorials-home/#interview-questions" data-ps2id-api="true">R Interview Questions</a></li><li id="menu-item-44880" class="intmenubi menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-44880"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#" data-ps2id-api="true">BI Interview Questions</a><ul class="sub-menu"><li id="menu-item-44860" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44860"><a href="https://data-flair.training/blogs/power-bi-tutorials-home/#interview-questions" data-ps2id-api="true">Power BI Interview Questions</a></li><li id="menu-item-44863" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44863"><a href="https://data-flair.training/blogs/tableau-tutorials-home/#interview-questions" data-ps2id-api="true">Tableau Interview Questions</a></li><li id="menu-item-44960" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44960"><a href="https://data-flair.training/blogs/qlikview-tutorials-home/#interview-questions" data-ps2id-api="true">QlikView Interview Questions</a></li></ul></li><li id="menu-item-44786" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44786"><a href="https://data-flair.training/blogs/java-tutorials-home/#interview-questions" data-ps2id-api="true">Java Interview Questions</a></li><li id="menu-item-44866" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44866"><a href="https://data-flair.training/blogs/sql-tutorials-home/#interview-questions" data-ps2id-api="true">SQL Interview Questions</a></li><li id="menu-item-44869" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-44869"><a href="https://data-flair.training/blogs/sas-tutorials-home/#interview-questions" data-ps2id-api="true">SAS Interview Questions</a></li></ul></li><li id="menu-item-48307" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-48307"><a href="https://data-flair.training/blogs/write-for-us/" data-ps2id-api="true">Write For Us</a></li></ul></div></nav><div class="container group"><div class="container-inner"><div class="group pad central-header-zone"><div class="logo-tagline-group"><p class="site-title"><a class="custom-logo-link" href="https://data-flair.training/blogs/" rel="home" title="DataFlair | Home page"><img src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/DataFlair_Logo_Final-01.png" alt="DataFlair"></a></p><p class="site-description">Learn Today. Lead Tomorrow.</p></div></div></div></div></header><div class="container" id="page"><div class="container-inner"><div class="main"><div class="main-inner group"><p id="breadcrumbs"><span><span><a href="https://data-flair.training/blogs/">Blog Home</a> » <span><a href="https://data-flair.training/blogs/category/python/">Python Tutorials</a> » <span class="breadcrumb_last" aria-current="page">Python based Project – Learn to Build Image Caption Generator with CNN &amp; LSTM</span></span></span></span></p><section class="content"><div class="page-title pad group"><ul class="meta-single group"><li class="category"><a href="https://data-flair.training/blogs/category/python/" rel="category tag">Python Tutorials</a></li><li class="comments"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comments"><i class="far fa-comments"></i>52</a></li></ul></div><div class="pad group"><article class="post-72771 post type-post status-publish format-standard has-post-thumbnail hentry category-python tag-advanced-python-project tag-image-caption-generator tag-python-based-project tag-python-data-science-project tag-python-project"><div class="post-inner group"><h1 class="post-title entry-title">Python based Project – Learn to Build Image Caption Generator with CNN &amp; LSTM</h1><p class="post-byline">by <span class="vcard author">
<span class="fn"><a href="https://data-flair.training/blogs/author/dfteam3/" title="Posts by DataFlair Team" rel="author">DataFlair Team</a></span>
</span> ·
Updated · <time class="updated" datetime="January 7, 2020">January 7, 2020</time></p><div class="clear"></div><div class="entry themeform"><div class="entry-inner"><p><strong>Project based on Python – Image Caption Generator&nbsp;</strong></p><p>You saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? Computer vision researchers worked on this a lot and they considered it impossible until now! With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image.</p><p>This is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together.</p><p>Below are some of the Python Data Science projects on which you can work later on:</p><ol><li><a href="https://data-flair.training/blogs/advanced-python-project-detecting-fake-news/">Fake News Detection Python Project</a></li><li><a href="https://data-flair.training/blogs/python-machine-learning-project-detecting-parkinson-disease/">Parkinson’s Disease Detection Python Project</a></li><li><a href="https://data-flair.training/blogs/project-in-python-colour-detection/">Color Detection Python Project</a></li><li><a href="https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/">Speech Emotion Recognition Python Project</a></li><li><a href="https://data-flair.training/blogs/project-in-python-breast-cancer-classification/">Breast Cancer Classification Python Project</a></li><li><a href="https://data-flair.training/blogs/python-project-gender-age-detection/">Age and Gender Detection Python Project</a></li><li><a href="https://data-flair.training/blogs/python-deep-learning-project-handwritten-digit-recognition/">Handwritten Digit Recognition Python Project</a></li><li><a href="https://data-flair.training/blogs/python-chatbot-project/">Chatbot Python Project</a></li><li><a href="https://data-flair.training/blogs/python-project-driver-drowsiness-detection-system/">Driver Drowsiness Detection Python Project</a></li><li><a href="https://data-flair.training/blogs/python-project-traffic-signs-recognition/">Traffic Signs Recognition Python Project</a></li><li>Image Caption Generator Python Project</li></ol><p>Now, let’s quickly start the Python based project by defining the image caption generator.</p><div class="code-block code-block-20 ai-viewport-2 ai-viewport-3" style="margin: 8px auto; text-align: center; display: block; clear: both;"><p><span style="font-weight:bold; font-style:italic;">
Keeping you updated with latest technology trends, <a href="https://t.me/dataflair">Join DataFlair on Telegram</a></span></p></div><h3>What is Image Caption Generator?</h3><p>Image caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English.</p><h3>Image Caption Generator with CNN – About the Python based Project</h3><p>The objective of our project is to learn the concepts of a CNN and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM.</p><p>In this Python project, we will be implementing the caption generator using <em><a href="https://data-flair.training/blogs/convolutional-neural-networks-tutorial/"><strong>CNN (Convolutional Neural Networks)</strong> </a></em>and LSTM (Long short term memory). The image features will be extracted from Xception which is a CNN model trained on the imagenet dataset and then we feed the features into the LSTM model which will be responsible for generating the image captions.</p><h3>The Dataset of Python based Project</h3><p>For the image caption generator, we will be using the Flickr_8K dataset. There are also other big datasets like Flickr_30K and MSCOCO dataset but it can take weeks just to train the network so we will be using a small Flickr8k dataset. The advantage of a huge dataset is that we can build better models.</p><p>Thanks to Jason Brownlee for providing a direct link to download the dataset (Size: 1GB).</p><ul><li><a href="https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip">Flicker8k_Dataset&nbsp;</a></li><li><a href="https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip">Flickr_8k_text&nbsp;</a></li></ul><p>The Flickr_8k_text folder contains file Flickr8k.token which is the main file of our dataset that contains image name and their respective captions separated by newline(“\n”).</p><h3>Pre-requisites</h3><p>This project requires good knowledge of Deep learning, Python, working on Jupyter notebooks, Keras library, Numpy, and <a href="https://data-flair.training/blogs/nlp-natural-language-processing/"><em><strong>Natural language processing</strong></em></a>.</p><p>Make sure you have installed all the following necessary libraries:</p><ul><li>pip install tensorflow</li><li>keras</li><li>pillow</li><li>numpy</li><li>tqdm</li><li>jupyterlab</li></ul><h2>Image Caption Generator – Python based Project</h2><h3>What is CNN?</h3><p>Convolutional Neural networks are specialized deep neural networks which can process the data that has input shape like a 2D matrix. Images are easily represented as a 2D matrix and CNN is very useful in working with images.</p><p>CNN is basically used for image classifications and identifying if an image is a bird, a plane or Superman, etc.</p><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project.png"><img class="aligncenter wp-image-72798 size-full" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project.png" alt="working of Deep CNN - Python based project" width="876" height="318" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project.png 876w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project-150x54.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project-300x109.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project-768x279.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/working-of-Deep-CNN-Python-project-520x189.png 520w" data-sizes="(max-width: 876px) 100vw, 876px"></a></p><p>It scans images from left to right and top to bottom to pull out important features from the image and combines the feature to classify images. It can handle the images that have been translated, rotated, scaled and changes in perspective.</p><p class="df-text-bold df-text-red" style="text-align: center">Practise the important Python topics</p><p class="df-text-bold" style="text-align: center">Check out the <a href="https://data-flair.training/blogs/python-tutorials-home/">240+ Python Tutorials</a></p><h3>What is LSTM?</h3><p>LSTM stands for <strong>Long short term memory</strong>, they are a type of RNN (<strong>recurrent neural network</strong>) which is well suited for sequence prediction problems. Based on the previous text, we can predict what the next word will be. It has proven itself effective from the traditional RNN by overcoming the limitations of RNN which had short term memory. LSTM can carry out relevant information throughout the processing of inputs and with a forget gate, it discards non-relevant information.</p><p>This is what an LSTM cell looks like –</p><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python.png"><img class="aligncenter size-full wp-image-72800" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python.png" alt="LSTM Cell Structure - simple python project" width="1300" height="853" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python.png 1300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python-150x98.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python-300x197.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python-768x504.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python-1024x672.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/LSTM-Cell-Structure-project-in-python-520x341.png 520w" data-sizes="(max-width: 1300px) 100vw, 1300px"></a></p><h3>Image Caption Generator Model</h3><p>So, to make our image caption generator model, we will be merging these architectures. It is also called a CNN-RNN model.</p><ul><li>CNN is used for extracting features from the image. We will use the pre-trained model Xception.</li><li>LSTM will use the information from CNN to help generate a description of the image.</li></ul><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project.png"><img class="aligncenter size-full wp-image-72812" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project.png" alt="Model of Image Caption Generator - python based project" width="1648" height="868" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project.png 1648w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project-150x79.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project-300x158.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project-768x405.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project-1024x539.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/Model-of-Image-Caption-Generator-python-project-520x274.png 520w" data-sizes="(max-width: 1648px) 100vw, 1648px"></a></p><h3>Project File Structure</h3><p>Downloaded from dataset:</p><ul><li><strong>Flicker8k_Dataset –</strong> Dataset folder which contains 8091 images.</li><li><strong>Flickr_8k_text –</strong> Dataset folder which contains text files and captions of images.</li></ul><p>The below files will be created by us while making the project.</p><ul><li><strong>Models –</strong> It will contain our trained models.</li><li><strong>Descriptions.txt –</strong> This text file contains all image names and their captions after preprocessing.</li><li><strong>Features.p –</strong> Pickle object that contains an image and their feature vector extracted from the Xception pre-trained CNN model.</li><li><strong>Tokenizer.p –</strong> Contains tokens mapped with an index value.</li><li><strong>Model.png –</strong> Visual representation of dimensions of our project.</li><li><strong>Testing_caption_generator.py –</strong> Python file for generating a caption of any image.</li><li><strong>Training_caption_generator.ipynb –</strong> Jupyter notebook in which we train and build our image caption generator.</li></ul><p>You can download all the files from the link:</p><p><a href="https://drive.google.com/open?id=13oJ_9jeylTmW7ivmuNmadwraWceHoQbK"><strong>Image Caption Generator – Python Project Files</strong></a></p><p><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project.png"><img class="aligncenter size-full wp-image-72801" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project.png" alt="structure - python based project" width="761" height="429" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project.png 761w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project-150x85.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project-300x169.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/structure-python-data-science-project-520x293.png 520w" data-sizes="(max-width: 761px) 100vw, 761px"></a></p><p class="df-text-bold df-text-red" style="text-align: center">Want to become a Python expert?</p><p class="df-text-bold" style="text-align: center">Enroll for the <a href="https://data-flair.training/python-course/">Certified Python Training Course</a></p><h3>Building the Python based Project</h3><p>Let’s start by initializing the jupyter notebook server by typing jupyter lab in the console of your project folder. It will open up the interactive Python notebook where you can run your code. Create a Python3 notebook and name it <strong>training_caption_generator.ipynb</strong></p><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1.jpg"><img class="aligncenter wp-image-74143 size-full" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1.jpg" alt="jupyter lab - python based project " width="846" height="517" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1.jpg 846w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1-150x92.jpg 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1-300x183.jpg 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1-768x469.jpg 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/jupyter-lab-advanced-python-project-1-520x318.jpg 520w" data-sizes="(max-width: 846px) 100vw, 846px"></a></p><p><strong>1. First, we import all the necessary packages</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">import string
import numpy as np
from PIL import Image
import os
from pickle import dump, load
import numpy as np

from keras.applications.xception import Xception, preprocess_input
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout

# small library for seeing the progress of loops.
from tqdm import tqdm_notebook as tqdm
tqdm().pandas()</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="">import string</span></li><li class=" even"><span class="">import numpy as np</span></li><li class=" odd"><span class="">from PIL import Image</span></li><li class=" even"><span class="">import os</span></li><li class=" odd"><span class="">from pickle import dump, load</span></li><li class=" even"><span class="">import numpy as np</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">from keras.applications.xception import Xception, preprocess_input</span></li><li class=" odd"><span class="">from keras.preprocessing.image import load_img, img_to_array</span></li><li class=" even"><span class="">from keras.preprocessing.text import Tokenizer</span></li><li class=" odd"><span class="">from keras.preprocessing.sequence import pad_sequences</span></li><li class=" even"><span class="">from keras.utils import to_categorical</span></li><li class=" odd"><span class="">from keras.layers.merge import add</span></li><li class=" even"><span class="">from keras.models import Model, load_model</span></li><li class=" odd"><span class="">from keras.layers import Input, Dense, LSTM, Embedding, Dropout</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1"># small library for seeing the progress of loops.</span><span class=""></span></li><li class=" even"><span class="">from tqdm import tqdm_notebook as tqdm</span></li><li class=" odd"><span class=""></span><span class="kw1">tqdm</span><span class="br0">(</span><span class="br0">)</span><span class="">.</span><span class="kw1">pandas</span><span class="br0">(</span><span class="br0">)</span></li></ol><pre style="display: none;">import string
import numpy as np
from PIL import Image
import os
from pickle import dump, load
import numpy as np

from keras.applications.xception import Xception, preprocess_input
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout

# small library for seeing the progress of loops.
from tqdm import tqdm_notebook as tqdm
tqdm().pandas()</pre></div><p><strong>2. Getting and performing data cleaning</strong></p><p>The main text file which contains all image captions is <strong>Flickr8k.token</strong> in our <strong>Flickr_8k_text</strong> folder.</p><p>Have a look at the file –</p><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python.png"><img class="aligncenter size-full wp-image-72803" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python.png" alt="token file - project in python" width="903" height="440" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python.png 903w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python-150x73.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python-300x146.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python-768x374.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/token-file-project-in-python-520x253.png 520w" data-sizes="(max-width: 903px) 100vw, 903px"></a></p><p>The format of our file is image and caption separated by a new line (“\n”).</p><p>Each image has 5 captions and we can see that #(0 to 5)number is assigned for each caption.</p><p>We will define 5 functions:</p><ul><li><strong>load_doc( filename ) –</strong> For loading the document file and reading the contents inside the file into a string.</li><li><strong>all_img_captions( filename ) –</strong> This function will create a <strong>descriptions</strong> dictionary that maps images with a list of 5 captions. The descriptions dictionary will look something like this:</li></ul><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1.png"><img class="aligncenter wp-image-72834 size-full" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1.png" alt="descriptions - python based project " width="1148" height="639" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1.png 1148w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1-150x83.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1-300x167.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1-768x427.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1-1024x570.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/descriptions-python-project-1-520x289.png 520w" data-sizes="(max-width: 1148px) 100vw, 1148px"></a></p><ul><li><strong>cleaning_text( descriptions) –</strong> This function takes all descriptions and performs data cleaning. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers.<br>
So, a caption like “A man riding on a three-wheeled wheelchair” will be transformed into “man riding on three wheeled wheelchair”</li><li><strong>text_vocabulary( descriptions ) –</strong> This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.</li><li><strong>save_descriptions( descriptions, filename ) –</strong> This function will create a list of all the descriptions that have been preprocessed and store them into a file. We will create a descriptions.txt file to store all the captions. It will look something like this:</li></ul><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project.png"><img class="aligncenter size-full wp-image-72805" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project.png" alt="save descriptions - python project" width="690" height="291" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project.png 690w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project-150x63.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project-300x127.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/save-descriptions-python-project-520x219.png 520w" data-sizes="(max-width: 690px) 100vw, 690px"></a></p><p><strong>Code :</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;"># Loading a text file into memory
def load_doc(filename):
    # Opening the file as read only
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

# get all imgs with their captions
def all_img_captions(filename):
    file = load_doc(filename)
    captions = file.split('\n')
    descriptions ={}
    for caption in captions[:-1]:
        img, caption = caption.split('\t')
        if img[:-2] not in descriptions:
            descriptions[img[:-2]] = 
        else:
            descriptions[img[:-2]].append(caption)
    return descriptions

#Data cleaning- lower casing, removing puntuations and words containing numbers
def cleaning_text(captions):
    table = str.maketrans('','',string.punctuation)
    for img,caps in captions.items():
        for i,img_caption in enumerate(caps):

            img_caption.replace("-"," ")
            desc = img_caption.split()

            #converts to lowercase
            desc = [word.lower() for word in desc]
            #remove punctuation from each token
            desc = [word.translate(table) for word in desc]
            #remove hanging 's and a 
            desc = [word for word in desc if(len(word)&gt;1)]
            #remove tokens with numbers in them
            desc = [word for word in desc if(word.isalpha())]
            #convert back to string

            img_caption = ' '.join(desc)
            captions[img][i]= img_caption
    return captions

def text_vocabulary(descriptions):
    # build vocabulary of all unique words
    vocab = set()

    for key in descriptions.keys():
        [vocab.update(d.split()) for d in descriptions[key]]

    return vocab

#All descriptions in one file 
def save_descriptions(descriptions, filename):
    lines = list()
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            lines.append(key + '\t' + desc )
    data = "\n".join(lines)
    file = open(filename,"w")
    file.write(data)
    file.close()


# Set these path according to project folder in you system
dataset_text = "D:\dataflair projects\Project - Image Caption Generator\Flickr_8k_text"
dataset_images = "D:\dataflair projects\Project - Image Caption Generator\Flicker8k_Dataset"

#we prepare our text data
filename = dataset_text + "/" + "Flickr8k.token.txt"
#loading the file that contains all data
#mapping them into descriptions dictionary img to 5 captions
descriptions = all_img_captions(filename)
print("Length of descriptions =" ,len(descriptions))

#cleaning the descriptions
clean_descriptions = cleaning_text(descriptions)

#building vocabulary 
vocabulary = text_vocabulary(clean_descriptions)
print("Length of vocabulary = ", len(vocabulary))

#saving each description to file 
save_descriptions(clean_descriptions, "descriptions.txt")</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1"># Loading a text file into memory</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">load_doc</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    </span><span class="co1"># Opening the file as read only</span><span class=""></span></li><li class=" even"><span class="">    file = </span><span class="kw1">open</span><span class="br0">(</span><span class="">filename, </span><span class="st0">'r'</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    text = file.</span><span class="kw1">read</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    file.</span><span class="kw1">close</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    return text</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1"># get all imgs with their captions</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">all_img_captions</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    file = </span><span class="kw1">load_doc</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    captions = file.</span><span class="kw1">split</span><span class="br0">(</span><span class="st0">'\n'</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    descriptions =</span><span class="br0">{</span><span class="br0">}</span><span class=""></span></li><li class=" even"><span class="">    for caption in captions</span><span class="br0">[</span><span class="">:-</span><span class="nu0">1</span><span class="br0">]</span><span class="">:</span></li><li class=" odd"><span class="">        img, caption = caption.</span><span class="kw1">split</span><span class="br0">(</span><span class="st0">'\t'</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        if img</span><span class="br0">[</span><span class="">:-</span><span class="nu0">2</span><span class="br0">]</span><span class=""> not in descriptions:</span></li><li class=" odd"><span class="">            descriptions</span><span class="br0">[</span><span class="">img</span><span class="br0">[</span><span class="">:-</span><span class="nu0">2</span><span class="br0">]</span><span class="br0">]</span><span class=""> = </span></li><li class=" even"><span class="">        else:</span></li><li class=" odd"><span class="">            descriptions</span><span class="br0">[</span><span class="">img</span><span class="br0">[</span><span class="">:-</span><span class="nu0">2</span><span class="br0">]</span><span class="br0">]</span><span class="">.</span><span class="kw1">append</span><span class="br0">(</span><span class="">caption</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    return descriptions</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#Data cleaning- lower casing, removing puntuations and words containing numbers</span><span class=""></span></li><li class=" odd"><span class="">def </span><span class="kw1">cleaning_text</span><span class="br0">(</span><span class="">captions</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">    table = str.</span><span class="kw1">maketrans</span><span class="br0">(</span><span class="st0">''</span><span class="">,</span><span class="st0">''</span><span class="">,string.punctuation</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    for img,caps in captions.</span><span class="kw1">items</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">        for i,img_caption in </span><span class="kw1">enumerate</span><span class="br0">(</span><span class="">caps</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">            img_caption.</span><span class="kw1">replace</span><span class="br0">(</span><span class="st0">"-"</span><span class="">,</span><span class="st0">" "</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">            desc = img_caption.</span><span class="kw1">split</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#converts to lowercase</span><span class=""></span></li><li class=" even"><span class="">            desc = </span><span class="br0">[</span><span class="">word.</span><span class="kw1">lower</span><span class="br0">(</span><span class="br0">)</span><span class=""> for word in desc</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#remove punctuation from each token</span><span class=""></span></li><li class=" even"><span class="">            desc = </span><span class="br0">[</span><span class="">word.</span><span class="kw1">translate</span><span class="br0">(</span><span class="">table</span><span class="br0">)</span><span class=""> for word in desc</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#remove hanging 's and a </span><span class=""></span></li><li class=" even"><span class="">            desc = </span><span class="br0">[</span><span class="">word for word in desc </span><span class="kw1">if</span><span class="br0">(</span><span class="kw1">len</span><span class="br0">(</span><span class="">word</span><span class="br0">)</span><span class="">&gt;</span><span class="nu0">1</span><span class="br0">)</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#remove tokens with numbers in them</span><span class=""></span></li><li class=" even"><span class="">            desc = </span><span class="br0">[</span><span class="">word for word in desc </span><span class="kw1">if</span><span class="br0">(</span><span class="">word.</span><span class="kw1">isalpha</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#convert back to string</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">            img_caption = </span><span class="st0">' '</span><span class="">.</span><span class="kw1">join</span><span class="br0">(</span><span class="">desc</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">            captions</span><span class="br0">[</span><span class="">img</span><span class="br0">]</span><span class="br0">[</span><span class="">i</span><span class="br0">]</span><span class="">= img_caption</span></li><li class=" odd"><span class="">    return captions</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">def </span><span class="kw1">text_vocabulary</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">    </span><span class="co1"># build vocabulary of all unique words</span><span class=""></span></li><li class=" odd"><span class="">    vocab = </span><span class="kw1">set</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">    for key in descriptions.</span><span class="kw1">keys</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">        </span><span class="br0">[</span><span class="">vocab.</span><span class="kw1">update</span><span class="br0">(</span><span class="">d.</span><span class="kw1">split</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class=""> for d in descriptions</span><span class="br0">[</span><span class="">key</span><span class="br0">]</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    return vocab</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#All descriptions in one file </span><span class=""></span></li><li class=" odd"><span class="">def </span><span class="kw1">save_descriptions</span><span class="br0">(</span><span class="">descriptions, filename</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">    lines = </span><span class="kw1">list</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    for key, desc_list in descriptions.</span><span class="kw1">items</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">        for desc in desc_list:</span></li><li class=" odd"><span class="">            lines.</span><span class="kw1">append</span><span class="br0">(</span><span class="">key + </span><span class="st0">'\t'</span><span class=""> + desc </span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    data = </span><span class="st0">"\n"</span><span class="">.</span><span class="kw1">join</span><span class="br0">(</span><span class="">lines</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    file = </span><span class="kw1">open</span><span class="br0">(</span><span class="">filename,</span><span class="st0">"w"</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    file.</span><span class="kw1">write</span><span class="br0">(</span><span class="">data</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    file.</span><span class="kw1">close</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1"># Set these path according to project folder in you system</span><span class=""></span></li><li class=" odd"><span class="">dataset_text = </span><span class="st0">"D:\dataflair projects\Project - Image Caption Generator\Flickr_8k_text"</span><span class=""></span></li><li class=" even"><span class="">dataset_images = </span><span class="st0">"D:\dataflair projects\Project - Image Caption Generator\Flicker8k_Dataset"</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#we prepare our text data</span><span class=""></span></li><li class=" odd"><span class="">filename = dataset_text + </span><span class="st0">"/"</span><span class=""> + </span><span class="st0">"Flickr8k.token.txt"</span><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#loading the file that contains all data</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#mapping them into descriptions dictionary img to 5 captions</span><span class=""></span></li><li class=" even"><span class="">descriptions = </span><span class="kw1">all_img_captions</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">"Length of descriptions ="</span><span class=""> ,</span><span class="kw1">len</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#cleaning the descriptions</span><span class=""></span></li><li class=" even"><span class="">clean_descriptions = </span><span class="kw1">cleaning_text</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#building vocabulary </span><span class=""></span></li><li class=" odd"><span class="">vocabulary = </span><span class="kw1">text_vocabulary</span><span class="br0">(</span><span class="">clean_descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">"Length of vocabulary = "</span><span class="">, </span><span class="kw1">len</span><span class="br0">(</span><span class="">vocabulary</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#saving each description to file </span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">save_descriptions</span><span class="br0">(</span><span class="">clean_descriptions, </span><span class="st0">"descriptions.txt"</span><span class="br0">)</span></li></ol><pre style="display: none;"># Loading a text file into memory
def load_doc(filename):
    # Opening the file as read only
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

# get all imgs with their captions
def all_img_captions(filename):
    file = load_doc(filename)
    captions = file.split('\n')
    descriptions ={}
    for caption in captions[:-1]:
        img, caption = caption.split('\t')
        if img[:-2] not in descriptions:
            descriptions[img[:-2]] = 
        else:
            descriptions[img[:-2]].append(caption)
    return descriptions

#Data cleaning- lower casing, removing puntuations and words containing numbers
def cleaning_text(captions):
    table = str.maketrans('','',string.punctuation)
    for img,caps in captions.items():
        for i,img_caption in enumerate(caps):

            img_caption.replace("-"," ")
            desc = img_caption.split()

            #converts to lowercase
            desc = [word.lower() for word in desc]
            #remove punctuation from each token
            desc = [word.translate(table) for word in desc]
            #remove hanging 's and a 
            desc = [word for word in desc if(len(word)&gt;1)]
            #remove tokens with numbers in them
            desc = [word for word in desc if(word.isalpha())]
            #convert back to string

            img_caption = ' '.join(desc)
            captions[img][i]= img_caption
    return captions

def text_vocabulary(descriptions):
    # build vocabulary of all unique words
    vocab = set()

    for key in descriptions.keys():
        [vocab.update(d.split()) for d in descriptions[key]]

    return vocab

#All descriptions in one file 
def save_descriptions(descriptions, filename):
    lines = list()
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            lines.append(key + '\t' + desc )
    data = "\n".join(lines)
    file = open(filename,"w")
    file.write(data)
    file.close()


# Set these path according to project folder in you system
dataset_text = "D:\dataflair projects\Project - Image Caption Generator\Flickr_8k_text"
dataset_images = "D:\dataflair projects\Project - Image Caption Generator\Flicker8k_Dataset"

#we prepare our text data
filename = dataset_text + "/" + "Flickr8k.token.txt"
#loading the file that contains all data
#mapping them into descriptions dictionary img to 5 captions
descriptions = all_img_captions(filename)
print("Length of descriptions =" ,len(descriptions))

#cleaning the descriptions
clean_descriptions = cleaning_text(descriptions)

#building vocabulary 
vocabulary = text_vocabulary(clean_descriptions)
print("Length of vocabulary = ", len(vocabulary))

#saving each description to file 
save_descriptions(clean_descriptions, "descriptions.txt")</pre></div><p><strong>3. Extracting the feature vector from all images&nbsp;</strong></p><p>This technique is also called transfer learning, we don’t have to do everything on our own, we use the pre-trained model that have been already trained on large datasets and extract the features from these models and use them for our tasks. We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. We can directly import this model from the keras.applications . Make sure you are connected to the internet as the weights get automatically downloaded. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.</p><p>model = Xception( include_top=False, pooling=’avg’ )</p><p>The function <strong>extract_features()</strong> will extract features for all images and we will map image names with their respective feature array. Then we will dump the features dictionary into a “features.p” pickle file.</p><p><strong>Code:</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">def extract_features(directory):
        model = Xception( include_top=False, pooling='avg' )
        features = {}
        for img in tqdm(os.listdir(directory)):
            filename = directory + "/" + img
            image = Image.open(filename)
            image = image.resize((299,299))
            image = np.expand_dims(image, axis=0)
            #image = preprocess_input(image)
            image = image/127.5
            image = image - 1.0

            feature = model.predict(image)
            features[img] = feature
        return features

#2048 feature vector
features = extract_features(dataset_images)
dump(features, open("features.p","wb"))</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="">def </span><span class="kw1">extract_features</span><span class="br0">(</span><span class="">directory</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">        model = </span><span class="kw1">Xception</span><span class="br0">(</span><span class=""> include_top=False, pooling=</span><span class="st0">'avg'</span><span class=""> </span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        features = </span><span class="br0">{</span><span class="br0">}</span><span class=""></span></li><li class=" even"><span class="">        for img in </span><span class="kw1">tqdm</span><span class="br0">(</span><span class="">os.</span><span class="kw1">listdir</span><span class="br0">(</span><span class="">directory</span><span class="br0">)</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">            filename = directory + </span><span class="st0">"/"</span><span class=""> + img</span></li><li class=" even"><span class="">            image = Image.</span><span class="kw1">open</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">            image = image.</span><span class="kw1">resize</span><span class="br0">(</span><span class="br0">(</span><span class="nu0">299</span><span class="">,</span><span class="nu0">299</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">            image = np.</span><span class="kw1">expand_dims</span><span class="br0">(</span><span class="">image, axis=</span><span class="nu0">0</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">            </span><span class="co1">#image = preprocess_input(image)</span><span class=""></span></li><li class=" even"><span class="">            image = image/</span><span class="nu0">127.5</span><span class=""></span></li><li class=" odd"><span class="">            image = image - </span><span class="nu0">1.0</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">            feature = model.</span><span class="kw1">predict</span><span class="br0">(</span><span class="">image</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">            features</span><span class="br0">[</span><span class="">img</span><span class="br0">]</span><span class=""> = feature</span></li><li class=" odd"><span class="">        return features</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#2048 feature vector</span><span class=""></span></li><li class=" even"><span class="">features = </span><span class="kw1">extract_features</span><span class="br0">(</span><span class="">dataset_images</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">dump</span><span class="br0">(</span><span class="">features, </span><span class="kw1">open</span><span class="br0">(</span><span class="st0">"features.p"</span><span class="">,</span><span class="st0">"wb"</span><span class="br0">)</span><span class="br0">)</span></li></ol><pre style="display: none;">def extract_features(directory):
        model = Xception( include_top=False, pooling='avg' )
        features = {}
        for img in tqdm(os.listdir(directory)):
            filename = directory + "/" + img
            image = Image.open(filename)
            image = image.resize((299,299))
            image = np.expand_dims(image, axis=0)
            #image = preprocess_input(image)
            image = image/127.5
            image = image - 1.0

            feature = model.predict(image)
            features[img] = feature
        return features

#2048 feature vector
features = extract_features(dataset_images)
dump(features, open("features.p","wb"))</pre></div><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features.png"><img class="aligncenter size-full wp-image-72806" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features.png" alt="extracting features - python based project" width="805" height="641" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features.png 805w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features-150x119.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features-300x239.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features-768x612.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/extracting_features-520x414.png 520w" data-sizes="(max-width: 805px) 100vw, 805px"></a></p><p>This process can take a lot of time depending on your system. I am using an Nvidia 1050 GPU for training purpose so it took me around 7 minutes for performing this task. However, if you are using CPU then this process might take 1-2 hours. You can comment out the code and directly load the features from our pickle file.</p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">features = load(open("features.p","rb"))</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="">features = </span><span class="kw1">load</span><span class="br0">(</span><span class="kw1">open</span><span class="br0">(</span><span class="st0">"features.p"</span><span class="">,</span><span class="st0">"rb"</span><span class="br0">)</span><span class="br0">)</span></li></ol><pre style="display: none;">features = load(open("features.p","rb"))</pre></div><p><strong>4. Loading dataset for Training the model</strong></p><p>In our <strong>Flickr_8k_test</strong> folder, we have <strong>Flickr_8k.trainImages.txt</strong> file that contains a list of 6000 image names that we will use for training.</p><p>For loading the training dataset, we need more functions:</p><ul><li><strong>load_photos( filename ) –</strong> This will load the text file in a string and will return the list of image names.</li><li><strong>load_clean_descriptions( filename, photos ) –</strong> This function will create a dictionary that contains captions for each photo from the list of photos. We also append the &lt;start&gt; and &lt;end&gt; identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.</li><li><strong>load_features(photos) –</strong> This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model.</li></ul><p><strong>Code :</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">#load the data 
def load_photos(filename):
    file = load_doc(filename)
    photos = file.split("\n")[:-1]
    return photos


def load_clean_descriptions(filename, photos): 
    #loading clean_descriptions
    file = load_doc(filename)
    descriptions = {}
    for line in file.split("\n"):

        words = line.split()
        if len(words)&lt;1 :
            continue

        image, image_caption = words[0], words[1:]

        if image in photos:
            if image not in descriptions:
                descriptions[image] = []
            desc = '&lt;start&gt; ' + " ".join(image_caption) + ' &lt;end&gt;'
            descriptions[image].append(desc)

    return descriptions


def load_features(photos):
    #loading all features
    all_features = load(open("features.p","rb"))
    #selecting only needed features
    features = {k:all_features[k] for k in photos}
    return features


filename = dataset_text + "/" + "Flickr_8k.trainImages.txt"

#train = loading_data(filename)
train_imgs = load_photos(filename)
train_descriptions = load_clean_descriptions("descriptions.txt", train_imgs)
train_features = load_features(train_imgs)</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1">#load the data </span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">load_photos</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    file = </span><span class="kw1">load_doc</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    photos = file.</span><span class="kw1">split</span><span class="br0">(</span><span class="st0">"\n"</span><span class="br0">)</span><span class="br0">[</span><span class="">:-</span><span class="nu0">1</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">    return photos</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">load_clean_descriptions</span><span class="br0">(</span><span class="">filename, photos</span><span class="br0">)</span><span class="">: </span></li><li class=" odd"><span class="">    </span><span class="co1">#loading clean_descriptions</span><span class=""></span></li><li class=" even"><span class="">    file = </span><span class="kw1">load_doc</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    descriptions = </span><span class="br0">{</span><span class="br0">}</span><span class=""></span></li><li class=" even"><span class="">    for line in file.</span><span class="kw1">split</span><span class="br0">(</span><span class="st0">"\n"</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">        words = line.</span><span class="kw1">split</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        if </span><span class="kw1">len</span><span class="br0">(</span><span class="">words</span><span class="br0">)</span><span class="">&lt;</span><span class="nu0">1</span><span class=""> :</span></li><li class=" even"><span class="">            continue</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">        image, image_caption = words</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="">, words</span><span class="br0">[</span><span class="nu0">1</span><span class="">:</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">        if image in photos:</span></li><li class=" odd"><span class="">            if image not in descriptions:</span></li><li class=" even"><span class="">                descriptions</span><span class="br0">[</span><span class="">image</span><span class="br0">]</span><span class=""> = </span><span class="br0">[</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            desc = </span><span class="st0">'&lt;start&gt; '</span><span class=""> + </span><span class="st0">" "</span><span class="">.</span><span class="kw1">join</span><span class="br0">(</span><span class="">image_caption</span><span class="br0">)</span><span class=""> + </span><span class="st0">' &lt;end&gt;'</span><span class=""></span></li><li class=" even"><span class="">            descriptions</span><span class="br0">[</span><span class="">image</span><span class="br0">]</span><span class="">.</span><span class="kw1">append</span><span class="br0">(</span><span class="">desc</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    return descriptions</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">def </span><span class="kw1">load_features</span><span class="br0">(</span><span class="">photos</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">    </span><span class="co1">#loading all features</span><span class=""></span></li><li class=" odd"><span class="">    all_features = </span><span class="kw1">load</span><span class="br0">(</span><span class="kw1">open</span><span class="br0">(</span><span class="st0">"features.p"</span><span class="">,</span><span class="st0">"rb"</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    </span><span class="co1">#selecting only needed features</span><span class=""></span></li><li class=" odd"><span class="">    features = </span><span class="br0">{</span><span class="">k:all_features</span><span class="br0">[</span><span class="">k</span><span class="br0">]</span><span class=""> for k in photos</span><span class="br0">}</span><span class=""></span></li><li class=" even"><span class="">    return features</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">filename = dataset_text + </span><span class="st0">"/"</span><span class=""> + </span><span class="st0">"Flickr_8k.trainImages.txt"</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#train = loading_data(filename)</span><span class=""></span></li><li class=" even"><span class="">train_imgs = </span><span class="kw1">load_photos</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">train_descriptions = </span><span class="kw1">load_clean_descriptions</span><span class="br0">(</span><span class="st0">"descriptions.txt"</span><span class="">, train_imgs</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">train_features = </span><span class="kw1">load_features</span><span class="br0">(</span><span class="">train_imgs</span><span class="br0">)</span></li></ol><pre style="display: none;">#load the data 
def load_photos(filename):
    file = load_doc(filename)
    photos = file.split("\n")[:-1]
    return photos


def load_clean_descriptions(filename, photos): 
    #loading clean_descriptions
    file = load_doc(filename)
    descriptions = {}
    for line in file.split("\n"):

        words = line.split()
        if len(words)&lt;1 :
            continue

        image, image_caption = words[0], words[1:]

        if image in photos:
            if image not in descriptions:
                descriptions[image] = []
            desc = '&lt;start&gt; ' + " ".join(image_caption) + ' &lt;end&gt;'
            descriptions[image].append(desc)

    return descriptions


def load_features(photos):
    #loading all features
    all_features = load(open("features.p","rb"))
    #selecting only needed features
    features = {k:all_features[k] for k in photos}
    return features


filename = dataset_text + "/" + "Flickr_8k.trainImages.txt"

#train = loading_data(filename)
train_imgs = load_photos(filename)
train_descriptions = load_clean_descriptions("descriptions.txt", train_imgs)
train_features = load_features(train_imgs)</pre></div><p><strong>5. Tokenizing the vocabulary&nbsp;</strong></p><p>Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a <strong>“tokenizer.p”</strong> pickle file.</p><p><strong>Code:</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">#converting dictionary to clean list of descriptions
def dict_to_list(descriptions):
    all_desc = []
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

#creating tokenizer class 
#this will vectorise text corpus
#each integer will represent token in dictionary

from keras.preprocessing.text import Tokenizer

def create_tokenizer(descriptions):
    desc_list = dict_to_list(descriptions)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(desc_list)
    return tokenizer

# give each word an index, and store that into tokenizer.p pickle file
tokenizer = create_tokenizer(train_descriptions)
dump(tokenizer, open('tokenizer.p', 'wb'))
vocab_size = len(tokenizer.word_index) + 1
vocab_size</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1">#converting dictionary to clean list of descriptions</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">dict_to_list</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    all_desc = </span><span class="br0">[</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">    for key in descriptions.</span><span class="kw1">keys</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">        </span><span class="br0">[</span><span class="">all_desc.</span><span class="kw1">append</span><span class="br0">(</span><span class="">d</span><span class="br0">)</span><span class=""> for d in descriptions</span><span class="br0">[</span><span class="">key</span><span class="br0">]</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">    return all_desc</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#creating tokenizer class </span><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#this will vectorise text corpus</span><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#each integer will represent token in dictionary</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">from keras.preprocessing.text import Tokenizer</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">create_tokenizer</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    desc_list = </span><span class="kw1">dict_to_list</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    tokenizer = </span><span class="kw1">Tokenizer</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    tokenizer.</span><span class="kw1">fit_on_texts</span><span class="br0">(</span><span class="">desc_list</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    return tokenizer</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1"># give each word an index, and store that into tokenizer.p pickle file</span><span class=""></span></li><li class=" odd"><span class="">tokenizer = </span><span class="kw1">create_tokenizer</span><span class="br0">(</span><span class="">train_descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">dump</span><span class="br0">(</span><span class="">tokenizer, </span><span class="kw1">open</span><span class="br0">(</span><span class="st0">'tokenizer.p'</span><span class="">, </span><span class="st0">'wb'</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">vocab_size = </span><span class="kw1">len</span><span class="br0">(</span><span class="">tokenizer.word_index</span><span class="br0">)</span><span class=""> + </span><span class="nu0">1</span><span class=""></span></li><li class=" even"><span class="">vocab_size</span></li></ol><pre style="display: none;">#converting dictionary to clean list of descriptions
def dict_to_list(descriptions):
    all_desc = []
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

#creating tokenizer class 
#this will vectorise text corpus
#each integer will represent token in dictionary

from keras.preprocessing.text import Tokenizer

def create_tokenizer(descriptions):
    desc_list = dict_to_list(descriptions)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(desc_list)
    return tokenizer

# give each word an index, and store that into tokenizer.p pickle file
tokenizer = create_tokenizer(train_descriptions)
dump(tokenizer, open('tokenizer.p', 'wb'))
vocab_size = len(tokenizer.word_index) + 1
vocab_size</pre></div><p>Our vocabulary contains 7577 words.</p><p>We calculate the maximum length of the descriptions. This is important for deciding the model structure parameters. Max_length of description is 32.</p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">#calculate maximum length of descriptions
def max_length(descriptions):
    desc_list = dict_to_list(descriptions)
    return max(len(d.split()) for d in desc_list)
    
max_length = max_length(descriptions)
max_length</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1">#calculate maximum length of descriptions</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">max_length</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    desc_list = </span><span class="kw1">dict_to_list</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    return </span><span class="kw1">max</span><span class="br0">(</span><span class="kw1">len</span><span class="br0">(</span><span class="">d.</span><span class="kw1">split</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class=""> for d in desc_list</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    </span></li><li class=" even"><span class="">max_length = </span><span class="kw1">max_length</span><span class="br0">(</span><span class="">descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">max_length</span></li></ol><pre style="display: none;">#calculate maximum length of descriptions
def max_length(descriptions):
    desc_list = dict_to_list(descriptions)
    return max(len(d.split()) for d in desc_list)
    
max_length = max_length(descriptions)
max_length</pre></div><p><strong>6. Create Data generator</strong></p><p>Let us first see how the input and output of our model will look like. To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.</p><p>The generator will yield the input and output sequence.</p><p><strong>For example:</strong></p><p>The input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict.</p><table><tbody><tr><td><span style="font-weight: 400">x1(feature vector)</span></td><td><span style="font-weight: 400">x2(Text sequence)</span></td><td><span style="font-weight: 400">y(word to predict)</span></td></tr><tr><td><span style="font-weight: 400">feature</span></td><td><span style="font-weight: 400">start,</span></td><td><span style="font-weight: 400">two</span></td></tr><tr><td><span style="font-weight: 400">feature</span></td><td><span style="font-weight: 400">start, two</span></td><td><span style="font-weight: 400">dogs</span></td></tr><tr><td><span style="font-weight: 400">feature</span></td><td><span style="font-weight: 400">start, two, dogs</span></td><td><span style="font-weight: 400">drink</span></td></tr><tr><td><span style="font-weight: 400">feature</span></td><td><span style="font-weight: 400">start, two, dogs, drink</span></td><td><span style="font-weight: 400">water</span></td></tr><tr><td><span style="font-weight: 400">feature</span></td><td><span style="font-weight: 400">start, two, dogs, drink, water</span></td><td><span style="font-weight: 400">end</span></td></tr></tbody></table><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">#create input-output sequence pairs from the image description.

#data generator, used by model.fit_generator()
def data_generator(descriptions, features, tokenizer, max_length):
    while 1:
        for key, description_list in descriptions.items():
            #retrieve photo features
            feature = features[key][0]
            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)
            yield [[input_image, input_sequence], output_word]

def create_sequences(tokenizer, max_length, desc_list, feature):
    X1, X2, y = list(), list(), list()
    # walk through each description for the image
    for desc in desc_list:
        # encode the sequence
        seq = tokenizer.texts_to_sequences([desc])[0]
        # split one sequence into multiple X,y pairs
        for i in range(1, len(seq)):
            # split into input and output pair
            in_seq, out_seq = seq[:i], seq[i]
            # pad input sequence
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            # encode output sequence
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            # store
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

#You can check the shape of the input and output for your model
[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))
a.shape, b.shape, c.shape
#((47, 2048), (47, 32), (47, 7577))</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1">#create input-output sequence pairs from the image description.</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1">#data generator, used by model.fit_generator()</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">data_generator</span><span class="br0">(</span><span class="">descriptions, features, tokenizer, max_length</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    while </span><span class="nu0">1</span><span class="">:</span></li><li class=" even"><span class="">        for key, description_list in descriptions.</span><span class="kw1">items</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">            </span><span class="co1">#retrieve photo features</span><span class=""></span></li><li class=" even"><span class="">            feature = features</span><span class="br0">[</span><span class="">key</span><span class="br0">]</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">            input_image, input_sequence, output_word = </span><span class="kw1">create_sequences</span><span class="br0">(</span><span class="">tokenizer, max_length, description_list, feature</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">            yield </span><span class="br0">[</span><span class="br0">[</span><span class="">input_image, input_sequence</span><span class="br0">]</span><span class="">, output_word</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">create_sequences</span><span class="br0">(</span><span class="">tokenizer, max_length, desc_list, feature</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    X1, X2, y = </span><span class="kw1">list</span><span class="br0">(</span><span class="br0">)</span><span class="">, </span><span class="kw1">list</span><span class="br0">(</span><span class="br0">)</span><span class="">, </span><span class="kw1">list</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    </span><span class="co1"># walk through each description for the image</span><span class=""></span></li><li class=" odd"><span class="">    for desc in desc_list:</span></li><li class=" even"><span class="">        </span><span class="co1"># encode the sequence</span><span class=""></span></li><li class=" odd"><span class="">        seq = tokenizer.</span><span class="kw1">texts_to_sequences</span><span class="br0">(</span><span class="br0">[</span><span class="">desc</span><span class="br0">]</span><span class="br0">)</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">        </span><span class="co1"># split one sequence into multiple X,y pairs</span><span class=""></span></li><li class=" odd"><span class="">        for i in </span><span class="kw1">range</span><span class="br0">(</span><span class="nu0">1</span><span class="">, </span><span class="kw1">len</span><span class="br0">(</span><span class="">seq</span><span class="br0">)</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">            </span><span class="co1"># split into input and output pair</span><span class=""></span></li><li class=" odd"><span class="">            in_seq, out_seq = seq</span><span class="br0">[</span><span class="">:i</span><span class="br0">]</span><span class="">, seq</span><span class="br0">[</span><span class="">i</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">            </span><span class="co1"># pad input sequence</span><span class=""></span></li><li class=" odd"><span class="">            in_seq = </span><span class="kw1">pad_sequences</span><span class="br0">(</span><span class="br0">[</span><span class="">in_seq</span><span class="br0">]</span><span class="">, maxlen=max_length</span><span class="br0">)</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">            </span><span class="co1"># encode output sequence</span><span class=""></span></li><li class=" odd"><span class="">            out_seq = </span><span class="kw1">to_categorical</span><span class="br0">(</span><span class="br0">[</span><span class="">out_seq</span><span class="br0">]</span><span class="">, num_classes=vocab_size</span><span class="br0">)</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">            </span><span class="co1"># store</span><span class=""></span></li><li class=" odd"><span class="">            X1.</span><span class="kw1">append</span><span class="br0">(</span><span class="">feature</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">            X2.</span><span class="kw1">append</span><span class="br0">(</span><span class="">in_seq</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">            y.</span><span class="kw1">append</span><span class="br0">(</span><span class="">out_seq</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    return np.</span><span class="kw1">array</span><span class="br0">(</span><span class="">X1</span><span class="br0">)</span><span class="">, np.</span><span class="kw1">array</span><span class="br0">(</span><span class="">X2</span><span class="br0">)</span><span class="">, np.</span><span class="kw1">array</span><span class="br0">(</span><span class="">y</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#You can check the shape of the input and output for your model</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="br0">[</span><span class="">a,b</span><span class="br0">]</span><span class="">,c = </span><span class="kw1">next</span><span class="br0">(</span><span class="kw1">data_generator</span><span class="br0">(</span><span class="">train_descriptions, features, tokenizer, max_length</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">a.shape, b.shape, c.shape</span></li><li class=" odd"><span class=""></span><span class="co1">#((47, 2048), (47, 32), (47, 7577))</span></li></ol><pre style="display: none;">#create input-output sequence pairs from the image description.

#data generator, used by model.fit_generator()
def data_generator(descriptions, features, tokenizer, max_length):
    while 1:
        for key, description_list in descriptions.items():
            #retrieve photo features
            feature = features[key][0]
            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)
            yield [[input_image, input_sequence], output_word]

def create_sequences(tokenizer, max_length, desc_list, feature):
    X1, X2, y = list(), list(), list()
    # walk through each description for the image
    for desc in desc_list:
        # encode the sequence
        seq = tokenizer.texts_to_sequences([desc])[0]
        # split one sequence into multiple X,y pairs
        for i in range(1, len(seq)):
            # split into input and output pair
            in_seq, out_seq = seq[:i], seq[i]
            # pad input sequence
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            # encode output sequence
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            # store
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

#You can check the shape of the input and output for your model
[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))
a.shape, b.shape, c.shape
#((47, 2048), (47, 32), (47, 7577))</pre></div><p><strong>7. Defining the CNN-RNN model</strong></p><p>To define the structure of the model, we will be using the Keras Model from Functional API. It will consist of three major parts:</p><ul><li><strong>Feature Extractor –</strong> The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.</li><li><strong>Sequence Processor –</strong> An embedding layer will handle the textual input, followed by the LSTM layer.</li><li><strong>Decoder –</strong> By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size.</li></ul><p>Visual representation of the final model is given below –</p><p style="text-align: center"><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project.png"><img class="aligncenter size-full wp-image-72807" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project.png" alt="final model - python data science project" width="829" height="737" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project.png 829w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project-150x133.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project-300x267.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project-768x683.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/model-python-machine-learning-project-520x462.png 520w" data-sizes="(max-width: 829px) 100vw, 829px"></a></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">from keras.utils import plot_model

# define the captioning model
def define_model(vocab_size, max_length):

    # features from the CNN model squeezed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    # Merging both models
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="">from keras.utils import plot_model</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1"># define the captioning model</span><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">define_model</span><span class="br0">(</span><span class="">vocab_size, max_length</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    </span><span class="co1"># features from the CNN model squeezed from 2048 to 256 nodes</span><span class=""></span></li><li class=" odd"><span class="">    inputs1 = </span><span class="kw1">Input</span><span class="br0">(</span><span class="">shape=</span><span class="br0">(</span><span class="nu0">2048</span><span class="">,</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    fe1 = </span><span class="kw1">Dropout</span><span class="br0">(</span><span class="nu0">0.5</span><span class="br0">)</span><span class="br0">(</span><span class="">inputs1</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    fe2 = </span><span class="kw1">Dense</span><span class="br0">(</span><span class="nu0">256</span><span class="">, activation=</span><span class="st0">'relu'</span><span class="br0">)</span><span class="br0">(</span><span class="">fe1</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">    </span><span class="co1"># LSTM sequence model</span><span class=""></span></li><li class=" even"><span class="">    inputs2 = </span><span class="kw1">Input</span><span class="br0">(</span><span class="">shape=</span><span class="br0">(</span><span class="">max_length,</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    se1 = </span><span class="kw1">Embedding</span><span class="br0">(</span><span class="">vocab_size, </span><span class="nu0">256</span><span class="">, mask_zero=True</span><span class="br0">)</span><span class="br0">(</span><span class="">inputs2</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    se2 = </span><span class="kw1">Dropout</span><span class="br0">(</span><span class="nu0">0.5</span><span class="br0">)</span><span class="br0">(</span><span class="">se1</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    se3 = </span><span class="kw1">LSTM</span><span class="br0">(</span><span class="nu0">256</span><span class="br0">)</span><span class="br0">(</span><span class="">se2</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">    </span><span class="co1"># Merging both models</span><span class=""></span></li><li class=" even"><span class="">    decoder1 = </span><span class="kw1">add</span><span class="br0">(</span><span class="br0">[</span><span class="">fe2, se3</span><span class="br0">]</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    decoder2 = </span><span class="kw1">Dense</span><span class="br0">(</span><span class="nu0">256</span><span class="">, activation=</span><span class="st0">'relu'</span><span class="br0">)</span><span class="br0">(</span><span class="">decoder1</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    outputs = </span><span class="kw1">Dense</span><span class="br0">(</span><span class="">vocab_size, activation=</span><span class="st0">'softmax'</span><span class="br0">)</span><span class="br0">(</span><span class="">decoder2</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    </span><span class="co1"># tie it together [image, seq] [word]</span><span class=""></span></li><li class=" odd"><span class="">    model = </span><span class="kw1">Model</span><span class="br0">(</span><span class="">inputs=</span><span class="br0">[</span><span class="">inputs1, inputs2</span><span class="br0">]</span><span class="">, outputs=outputs</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    model.</span><span class="kw1">compile</span><span class="br0">(</span><span class="">loss=</span><span class="st0">'categorical_crossentropy'</span><span class="">, optimizer=</span><span class="st0">'adam'</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    </span><span class="co1"># summarize model</span><span class=""></span></li><li class=" odd"><span class="">    </span><span class="kw1">print</span><span class="br0">(</span><span class="">model.</span><span class="kw1">summary</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    </span><span class="kw1">plot_model</span><span class="br0">(</span><span class="">model, to_file=</span><span class="st0">'model.png'</span><span class="">, show_shapes=True</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">    return model</span></li></ol><pre style="display: none;">from keras.utils import plot_model

# define the captioning model
def define_model(vocab_size, max_length):

    # features from the CNN model squeezed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    # Merging both models
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # tie it together [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model</pre></div><p><strong>8. Training the model</strong></p><div class="code-block code-block-14" style="margin: 8px auto; text-align: center; display: block; clear: both;">
<a href="https://data-flair.training/blogs/top-python-interview-questions-answer/" target="_blank"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/02/Python-interview-banner-01.jpg" alt="Python Interview Questions"></a></div><p>To train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability.</p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;"># train our model
print('Dataset: ', len(train_imgs))
print('Descriptions: train=', len(train_descriptions))
print('Photos: train=', len(train_features))
print('Vocabulary Size:', vocab_size)
print('Description Length: ', max_length)

model = define_model(vocab_size, max_length)
epochs = 10
steps = len(train_descriptions)
# making a directory models to save our models
os.mkdir("models")
for i in range(epochs):
    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)
    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)
    model.save("models/model_" + str(i) + ".h5")</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="co1"># train our model</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">'Dataset: '</span><span class="">, </span><span class="kw1">len</span><span class="br0">(</span><span class="">train_imgs</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">'Descriptions: train='</span><span class="">, </span><span class="kw1">len</span><span class="br0">(</span><span class="">train_descriptions</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">'Photos: train='</span><span class="">, </span><span class="kw1">len</span><span class="br0">(</span><span class="">train_features</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">'Vocabulary Size:'</span><span class="">, vocab_size</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">'Description Length: '</span><span class="">, max_length</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">model = </span><span class="kw1">define_model</span><span class="br0">(</span><span class="">vocab_size, max_length</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">epochs = </span><span class="nu0">10</span><span class=""></span></li><li class=" even"><span class="">steps = </span><span class="kw1">len</span><span class="br0">(</span><span class="">train_descriptions</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="co1"># making a directory models to save our models</span><span class=""></span></li><li class=" even"><span class="">os.</span><span class="kw1">mkdir</span><span class="br0">(</span><span class="st0">"models"</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">for i in </span><span class="kw1">range</span><span class="br0">(</span><span class="">epochs</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">    generator = </span><span class="kw1">data_generator</span><span class="br0">(</span><span class="">train_descriptions, train_features, tokenizer, max_length</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">    model.</span><span class="kw1">fit_generator</span><span class="br0">(</span><span class="">generator, epochs=</span><span class="nu0">1</span><span class="">, steps_per_epoch= steps, verbose=</span><span class="nu0">1</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">    model.</span><span class="kw1">save</span><span class="br0">(</span><span class="st0">"models/model_"</span><span class=""> + </span><span class="kw1">str</span><span class="br0">(</span><span class="">i</span><span class="br0">)</span><span class=""> + </span><span class="st0">".h5"</span><span class="br0">)</span></li></ol><pre style="display: none;"># train our model
print('Dataset: ', len(train_imgs))
print('Descriptions: train=', len(train_descriptions))
print('Photos: train=', len(train_features))
print('Vocabulary Size:', vocab_size)
print('Description Length: ', max_length)

model = define_model(vocab_size, max_length)
epochs = 10
steps = len(train_descriptions)
# making a directory models to save our models
os.mkdir("models")
for i in range(epochs):
    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)
    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)
    model.save("models/model_" + str(i) + ".h5")</pre></div><p><strong>9. Testing the model</strong></p><p>The model has been trained, now, we will make a separate file testing_caption_generator.py which will load the model and generate predictions. The predictions contain the max length of index values so we will use the same tokenizer.p pickle file to get the words from their index values.</p><p><strong>Code:</strong></p><pre class="EnlighterJSRAW" data-enlighter-language="null" style="display: none;">import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse


ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help="Image Path")
args = vars(ap.parse_args())
img_path = args['image']

def extract_features(filename, model):
        try:
            image = Image.open(filename)

        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        image = image.resize((299,299))
        image = np.array(image)
        # for images that has 4 channels, we convert them into 3 channels
        if image.shape[2] == 4: 
            image = image[..., :3]
        image = np.expand_dims(image, axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        return feature

def word_for_id(integer, tokenizer):
for word, index in tokenizer.word_index.items():
     if index == integer:
         return word
return None


def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict([photo,sequence], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text


#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'
max_length = 32
tokenizer = load(open("tokenizer.p","rb"))
model = load_model('models/model_9.h5')
xception_model = Xception(include_top=False, pooling="avg")

photo = extract_features(img_path, xception_model)
img = Image.open(img_path)

description = generate_desc(model, tokenizer, photo, max_length)
print("\n\n")
print(description)
plt.imshow(img)

</pre><div class="EnlighterJSWrapper eclipseEnlighterJSWrapper"><ol class="hoverEnabled eclipseEnlighterJS EnlighterJS"><li class=" odd"><span class="">import numpy as np</span></li><li class=" even"><span class="">from PIL import Image</span></li><li class=" odd"><span class="">import matplotlib.pyplot as plt</span></li><li class=" even"><span class="">import argparse</span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">ap = argparse.</span><span class="kw1">ArgumentParser</span><span class="br0">(</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">ap.</span><span class="kw1">add_argument</span><span class="br0">(</span><span class="st0">'-i'</span><span class="">, </span><span class="st0">'--image'</span><span class="">, required=True, help=</span><span class="st0">"Image Path"</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">args = </span><span class="kw1">vars</span><span class="br0">(</span><span class="">ap.</span><span class="kw1">parse_args</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">img_path = args</span><span class="br0">[</span><span class="st0">'image'</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">extract_features</span><span class="br0">(</span><span class="">filename, model</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">        try:</span></li><li class=" even"><span class="">            image = Image.</span><span class="kw1">open</span><span class="br0">(</span><span class="">filename</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">        except:</span></li><li class=" odd"><span class="">            </span><span class="kw1">print</span><span class="br0">(</span><span class="st0">"ERROR: Couldn't open image! Make sure the image path and extension is correct"</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        image = image.</span><span class="kw1">resize</span><span class="br0">(</span><span class="br0">(</span><span class="nu0">299</span><span class="">,</span><span class="nu0">299</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        image = np.</span><span class="kw1">array</span><span class="br0">(</span><span class="">image</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        </span><span class="co1"># for images that has 4 channels, we convert them into 3 channels</span><span class=""></span></li><li class=" odd"><span class="">        if image.shape</span><span class="br0">[</span><span class="nu0">2</span><span class="br0">]</span><span class=""> == </span><span class="nu0">4</span><span class="">: </span></li><li class=" even"><span class="">            image = image</span><span class="br0">[</span><span class="">..., :</span><span class="nu0">3</span><span class="br0">]</span><span class=""></span></li><li class=" odd"><span class="">        image = np.</span><span class="kw1">expand_dims</span><span class="br0">(</span><span class="">image, axis=</span><span class="nu0">0</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        image = image/</span><span class="nu0">127.5</span><span class=""></span></li><li class=" odd"><span class="">        image = image - </span><span class="nu0">1.0</span><span class=""></span></li><li class=" even"><span class="">        feature = model.</span><span class="kw1">predict</span><span class="br0">(</span><span class="">image</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        return feature</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">def </span><span class="kw1">word_for_id</span><span class="br0">(</span><span class="">integer, tokenizer</span><span class="br0">)</span><span class="">:</span></li><li class=" even"><span class="">for word, index in tokenizer.word_index.</span><span class="kw1">items</span><span class="br0">(</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">     if index == integer:</span></li><li class=" even"><span class="">         return word</span></li><li class=" odd"><span class="">return None</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">def </span><span class="kw1">generate_desc</span><span class="br0">(</span><span class="">model, tokenizer, photo, max_length</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">    in_text = </span><span class="st0">'start'</span><span class=""></span></li><li class=" even"><span class="">    for i in </span><span class="kw1">range</span><span class="br0">(</span><span class="">max_length</span><span class="br0">)</span><span class="">:</span></li><li class=" odd"><span class="">        sequence = tokenizer.</span><span class="kw1">texts_to_sequences</span><span class="br0">(</span><span class="br0">[</span><span class="">in_text</span><span class="br0">]</span><span class="br0">)</span><span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class=""></span></li><li class=" even"><span class="">        sequence = </span><span class="kw1">pad_sequences</span><span class="br0">(</span><span class="br0">[</span><span class="">sequence</span><span class="br0">]</span><span class="">, maxlen=max_length</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        pred = model.</span><span class="kw1">predict</span><span class="br0">(</span><span class="br0">[</span><span class="">photo,sequence</span><span class="br0">]</span><span class="">, verbose=</span><span class="nu0">0</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        pred = np.</span><span class="kw1">argmax</span><span class="br0">(</span><span class="">pred</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">        word = </span><span class="kw1">word_for_id</span><span class="br0">(</span><span class="">pred, tokenizer</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">        if word is None:</span></li><li class=" odd"><span class="">            break</span></li><li class=" even"><span class="">        in_text += </span><span class="st0">' '</span><span class=""> + word</span></li><li class=" odd"><span class="">        if word == </span><span class="st0">'end'</span><span class="">:</span></li><li class=" even"><span class="">            break</span></li><li class=" odd"><span class="">    return in_text</span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class=""></span><span class="co1">#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'</span><span class=""></span></li><li class=" odd"><span class="">max_length = </span><span class="nu0">32</span><span class=""></span></li><li class=" even"><span class="">tokenizer = </span><span class="kw1">load</span><span class="br0">(</span><span class="kw1">open</span><span class="br0">(</span><span class="st0">"tokenizer.p"</span><span class="">,</span><span class="st0">"rb"</span><span class="br0">)</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">model = </span><span class="kw1">load_model</span><span class="br0">(</span><span class="st0">'models/model_9.h5'</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">xception_model = </span><span class="kw1">Xception</span><span class="br0">(</span><span class="">include_top=False, pooling=</span><span class="st0">"avg"</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span></li><li class=" even"><span class="">photo = </span><span class="kw1">extract_features</span><span class="br0">(</span><span class="">img_path, xception_model</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class="">img = Image.</span><span class="kw1">open</span><span class="br0">(</span><span class="">img_path</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span></li><li class=" odd"><span class="">description = </span><span class="kw1">generate_desc</span><span class="br0">(</span><span class="">model, tokenizer, photo, max_length</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="st0">"\n\n"</span><span class="br0">)</span><span class=""></span></li><li class=" odd"><span class=""></span><span class="kw1">print</span><span class="br0">(</span><span class="">description</span><span class="br0">)</span><span class=""></span></li><li class=" even"><span class="">plt.</span><span class="kw1">imshow</span><span class="br0">(</span><span class="">img</span><span class="br0">)</span></li></ol><pre style="display: none;">import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import argparse


ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help="Image Path")
args = vars(ap.parse_args())
img_path = args['image']

def extract_features(filename, model):
        try:
            image = Image.open(filename)

        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        image = image.resize((299,299))
        image = np.array(image)
        # for images that has 4 channels, we convert them into 3 channels
        if image.shape[2] == 4: 
            image = image[..., :3]
        image = np.expand_dims(image, axis=0)
        image = image/127.5
        image = image - 1.0
        feature = model.predict(image)
        return feature

def word_for_id(integer, tokenizer):
for word, index in tokenizer.word_index.items():
     if index == integer:
         return word
return None


def generate_desc(model, tokenizer, photo, max_length):
    in_text = 'start'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict([photo,sequence], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'end':
            break
    return in_text


#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'
max_length = 32
tokenizer = load(open("tokenizer.p","rb"))
model = load_model('models/model_9.h5')
xception_model = Xception(include_top=False, pooling="avg")

photo = extract_features(img_path, xception_model)
img = Image.open(img_path)

description = generate_desc(model, tokenizer, photo, max_length)
print("\n\n")
print(description)
plt.imshow(img)</pre></div><p><strong>Results:</strong></p><p><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock.png"><img class="aligncenter size-full wp-image-72819" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock.png" alt="image caption generator - man standing on rock" width="1366" height="522" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock.png 1366w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock-150x57.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock-300x115.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock-768x293.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock-1024x391.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-standing-on-rock-520x199.png 520w" data-sizes="(max-width: 1366px) 100vw, 1366px"></a></p><p><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing.png"><img class="aligncenter size-full wp-image-72820" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing.png" alt="image caption generator - girls playing" width="1366" height="532" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing.png 1366w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing-150x58.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing-300x117.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing-768x299.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing-1024x399.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-girls-playing-520x203.png 520w" data-sizes="(max-width: 1366px) 100vw, 1366px"></a></p><p><a href="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak.png"><img class="aligncenter size-full wp-image-72821" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak.png" alt="python project on image caption generator - man on kayak" width="1366" height="531" data-srcset="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak.png 1366w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak-150x58.png 150w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak-300x117.png 300w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak-768x299.png 768w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak-1024x398.png 1024w, https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/11/image-caption-generator-man-on-kayak-520x202.png 520w" data-sizes="(max-width: 1366px) 100vw, 1366px"></a></p><h2>Summary</h2><p>In this advanced Python project, we have implemented a CNN-RNN model by building an image caption generator. Some key points to note are that our model depends on the data, so, it cannot predict the words that are out of its vocabulary. We used a small dataset consisting of 8000 images. For production-level models, we need to train on datasets larger than 100,000 images which can produce better accuracy models.</p><p class="df-text-bold df-text-red" style="text-align: center">Rock the Python interview round</p><p class="df-text-bold" style="text-align: center">Practise <a href="https://data-flair.training/blogs/top-python-interview-questions-answer/">150+ Python Interview Questions</a></p><p>Hope you enjoyed making this Python based project with us. You can ask your doubts in the comment section below.</p><nav class="pagination group"></nav></div><div class="clear"></div></div></div></article><div class="clear"></div><p class="post-tags"><span>Tags:</span> <a href="https://data-flair.training/blogs/tag/advanced-python-project/" rel="tag">Advanced python project</a><a href="https://data-flair.training/blogs/tag/image-caption-generator/" rel="tag">Image Caption Generator</a><a href="https://data-flair.training/blogs/tag/python-based-project/" rel="tag">python based project</a><a href="https://data-flair.training/blogs/tag/python-data-science-project/" rel="tag">Python data science project</a><a href="https://data-flair.training/blogs/tag/python-project/" rel="tag">Python project</a></p> <script>jQuery( function($) {
              czrapp.proRelPostsRendered = $.Deferred();
              var waypoint = new Waypoint({
                  element: document.getElementById('pro-related-posts-wrapper'),
                  handler: function(direction) {
                        if ( 'pending' == czrapp.proRelPostsRendered.state() ) {
                              var $wrap = $('#pro-related-posts-wrapper');
                              $wrap.addClass('loading');
                              czrapp.doAjax( {
                                      action: "ha_inject_pro_related",
                                      // => Always get the option from the $_POSTED data in ajax
                                      related_post_id : 72771,
                                      pro_related_posts_opt : {"id":"pro_related_posts_czr_module","title":"","enable":true,"col_number":3,"display_heading":true,"heading_text":"You may also like...","freescroll":true,"ajax_enabled":true,"post_number":"3","order_by":"rand","related_by":"categories"},
                                      free_related_posts_opt : "categories",
                                      layout_class : "col-3cm"
                                  } ).done( function( r ) {
                                        if ( r && r.data && r.data.html ) {
                                            if ( 'pending' == czrapp.proRelPostsRendered.state() ) {
                                                $.when( $('#pro-related-posts-wrapper').append( r.data.html ) ).done( function() {
                                                      czrapp.proRelPostsRendered.resolve();
                                                      $wrap.find('.czr-css-loader').css('opacity', 0);
                                                      _.delay( function() {
                                                            $wrap.removeClass('loading').addClass('loaded');
                                                      }, 800 );
                                                });
                                            }
                                        }
                                  });
                        }
                  },
                  offset: '110%'
              });
        });//jQuery()</script> <div id="pro-related-posts-wrapper"><div class="czr-css-loader czr-mr-loader dark"><div></div><div></div><div></div></div></div><section id="comments" class="themeform"><h3 class="heading">52 Responses</h3><ul class="comment-tabs group"><li class="active"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#commentlist-container"><i class="far fa-comments"></i>Comments<span>5</span></a></li><li><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#pinglist-container"><i class="fas fa-share"></i>Pingbacks<span>0</span></a></li></ul><div id="commentlist-container" class="comment-tab"><ol class="commentlist"><li class="comment even thread-even depth-1" id="comment-55290"><div id="div-comment-55290" class="comment-body"><div class="comment-author vcard">
<cite class="fn">Altaf</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-2/#comment-55290">
May 11, 2020 at 12:26 am</a></div><p>hey Everything works fine but atlast it’s showing this error its a raw code but I am using tensorflow as a backend—–<br>
ValueError                                Traceback (most recent call last)<br>
in<br>
13 for i in range(epochs):<br>
14     generator = data_generator(train_descriptions, train_features, tokenizer, max_length)<br>
—&gt; 15     model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)<br>
16     model.save(“models/model_” + str(i) + “.h5”)</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)<br>
1295         shuffle=shuffle,<br>
1296         initial_epoch=initial_epoch,<br>
-&gt; 1297         steps_name=’steps_per_epoch’)<br>
1298<br>
1299   def evaluate_generator(self,</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)<br>
263<br>
264       is_deferred = not model._is_compiled<br>
–&gt; 265       batch_outs = batch_function(*batch_data)<br>
266       if not isinstance(batch_outs, list):<br>
267         batch_outs = [batch_outs]</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)<br>
971       outputs = training_v2_utils.train_on_batch(<br>
972           self, x, y=y, sample_weight=sample_weight,<br>
–&gt; 973           class_weight=class_weight, reset_metrics=reset_metrics)<br>
974       outputs = (outputs[‘total_loss’] + outputs[‘output_losses’] +<br>
975                  outputs[‘metrics’])</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)<br>
251   x, y, sample_weights = model._standardize_user_data(<br>
252       x, y, sample_weight=sample_weight, class_weight=class_weight,<br>
–&gt; 253       extract_tensors_from_dataset=True)<br>
254   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]<br>
255   # If `model._distribution_strategy` is True, then we are in a replica context</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)<br>
2470           feed_input_shapes,<br>
2471           check_batch_axis=False,  # Don’t enforce the batch size.<br>
-&gt; 2472           exception_prefix=’input’)<br>
2473<br>
2474     # Get typespecs for the input data and sanitize it if necessary.</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)<br>
504   elif isinstance(data, (list, tuple)):<br>
505     if isinstance(data[0], (list, tuple)):<br>
–&gt; 506       data = [np.asarray(d) for d in data]<br>
507     elif len(names) == 1 and isinstance(data[0], (float, int)):<br>
508       data = [np.asarray(data)]</p><p>~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in (.0)<br>
504   elif isinstance(data, (list, tuple)):<br>
505     if isinstance(data[0], (list, tuple)):<br>
–&gt; 506       data = [np.asarray(d) for d in data]<br>
507     elif len(names) == 1 and isinstance(data[0], (float, int)):<br>
508       data = [np.asarray(data)]</p><p>~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)<br>
536<br>
537     “””<br>
–&gt; 538     return array(a, dtype, copy=False, order=order)<br>
539<br>
540</p><p>ValueError: could not broadcast input array from shape (47,2048) into shape (47)</p><div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comment-55290" onclick="return addComment.moveForm( &quot;div-comment-55290&quot;, &quot;55290&quot;, &quot;respond&quot;, &quot;72771&quot; )" aria-label="Reply to Altaf">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-55318"><div id="div-comment-55318" class="comment-body"><div class="comment-author vcard">
<cite class="fn">Pranshi Garg</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-2/#comment-55318">
May 14, 2020 at 6:37 pm</a></div><p>PermissionError                           Traceback (most recent call last)<br>
in<br>
1 directory =”D:\Flickr8k_Dataset”<br>
—-&gt; 2 features = extract_features(directory)<br>
3 print(‘Extracted Features: %d’ % len(features))<br>
4 # save to file<br>
5 dump(features, open(r’features.pkl’, ‘rb’))</p><p> in extract_features(directory)<br>
13                 # load an image from file<br>
14                 filename = directory + ‘/’ + name<br>
—&gt; 15                 image = load_img(filename, target_size=(224, 224))<br>
16                 # convert the image pixels to a numpy array<br>
17                 image = img_to_array(image)</p><p>~\anaconda3\lib\site-packages\keras_preprocessing\image\utils.py in load_img(path, grayscale, color_mode, target_size, interpolation)<br>
108         raise ImportError(‘Could not import PIL.Image. ‘<br>
109                           ‘The use of `load_img` requires PIL.’)<br>
–&gt; 110     img = pil_image.open(path)<br>
111     if color_mode == ‘grayscale’:<br>
112         if img.mode != ‘L’:</p><p>~\anaconda3\lib\site-packages\PIL\Image.py in open(fp, mode)<br>
2807<br>
2808     if filename:<br>
-&gt; 2809         fp = builtins.open(filename, “rb”)<br>
2810         exclusive_fp = True<br>
2811</p><p>PermissionError: [Errno 13] Permission denied: ‘D:\\Flickr8k_Dataset/Flicker8k_Dataset’</p><p>why is this error showing?can you please help me?</p><div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comment-55318" onclick="return addComment.moveForm( &quot;div-comment-55318&quot;, &quot;55318&quot;, &quot;respond&quot;, &quot;72771&quot; )" aria-label="Reply to Pranshi Garg">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-55346"><div id="div-comment-55346" class="comment-body"><div class="comment-author vcard">
<cite class="fn">Nandan Kalaria</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-2/#comment-55346">
May 18, 2020 at 12:06 am</a></div><p>How to measure the accuracy of the given model/project?</p><div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comment-55346" onclick="return addComment.moveForm( &quot;div-comment-55346&quot;, &quot;55346&quot;, &quot;respond&quot;, &quot;72771&quot; )" aria-label="Reply to Nandan Kalaria">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-55363"><div id="div-comment-55363" class="comment-body"><div class="comment-author vcard">
<cite class="fn">ZIDANE SUGIHARTO</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-2/#comment-55363">
May 21, 2020 at 2:54 am</a></div><p>how do use this program using bleu score for testing the accuracy of image</p><div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comment-55363" onclick="return addComment.moveForm( &quot;div-comment-55363&quot;, &quot;55363&quot;, &quot;respond&quot;, &quot;72771&quot; )" aria-label="Reply to ZIDANE SUGIHARTO">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-55415"><div id="div-comment-55415" class="comment-body"><div class="comment-author vcard">
<cite class="fn">Sumedh</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-2/#comment-55415">
May 26, 2020 at 11:44 am</a></div><p>The captions that are being generated are not accurate enough as shown in the result section of this page. What can i do to improve?</p><div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#comment-55415" onclick="return addComment.moveForm( &quot;div-comment-55415&quot;, &quot;55415&quot;, &quot;respond&quot;, &quot;72771&quot; )" aria-label="Reply to Sumedh">Reply</a></div></div></li></ol><nav class="comments-nav group"><div class="nav-previous"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/comment-page-1/#comments">« Older Comments</a></div><div class="nav-next"></div></nav></div><div id="respond" class="comment-respond"><h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#respond" style="display:none;">Cancel reply</a></small></h3><form action="https://data-flair.training/blogs/wp-comments-post.php" method="post" id="commentform" class="comment-form"><p class="comment-notes"><span id="email-notes">Your email address will not be published.</span> Required fields are marked <span class="required">*</span></p><p class="comment-form-comment"><label for="comment">Comment</label><textarea id="comment" name="comment" cols="45" rows="8" maxlength="65525" required="required"></textarea></p><p class="comment-form-author"><label for="author">Name <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" maxlength="245" required="required"></p><p class="comment-form-email"><label for="email">Email <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" maxlength="100" aria-describedby="email-notes" required="required"></p><p class="comment-form-url"><label for="url">Website</label> <input id="url" name="url" type="text" value="" size="30" maxlength="200"></p><div class="gglcptch gglcptch_v3"><div>This site is protected by reCAPTCHA and the Google <a href="https://policies.google.com/privacy">Privacy Policy</a> and <a href="https://policies.google.com/terms">Terms of Service</a> apply.</div><input type="hidden" id="g-recaptcha-response" name="g-recaptcha-response"> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/api.js.download"></script> <script>grecaptcha.ready(function() {
                                  grecaptcha.execute('6LdD0JwUAAAAAABRdMQUEDhUkj_9jYOOLjwoOiqV', {action: 'BWS_reCaptcha'}).then(function(token) {
                                     document.getElementById('g-recaptcha-response').value=token;
                                  });
                              });</script></div><p class="form-submit"><input name="submit" type="submit" id="submit" class="submit" value="Post Comment"> <input type="hidden" name="comment_post_ID" value="72771" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0"></p><p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="0feb8f57df"></p><p style="display: none;"><input type="hidden" id="ak_js" name="ak_js" value="87"></p></form></div></section></div></section><div class="sidebar s1 collapsed" data-position="left" data-layout="col-3cm" data-sb-id="s1"><a class="sidebar-toggle" title="Expand Sidebar"><i class="fas icon-sidebar-toggle"></i></a><div class="sidebar-content"><div id="pagesinwidgets_page_section-71" class="widget pagesinwidgets_page_section"><div class="homepage_section"><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python"><span class="dfacc-label-title">Python Tutorials</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-tutorial/">Python – Introduction</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/features-of-python/">Python – Features</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/advantages-and-disadvantages-of-python/">Python – Pros and Cons</a></li><li class="dfacc-lsbli dfblognew"><a href="https://data-flair.training/blogs/learn-python-notes/">Python – Master Guide</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-best-practices/">Python – Best Practices</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/why-learn-python/">Python – Reasons to Learn</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-tools/">Python – Tools</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/whats-new-in-python/">Python 3.8 Features</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/install-python-windows/">Python – Install on Windows</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/advantages-of-python-over-java/">Python – Advantages over Java</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-syntax-semantics/">Python – Syntax</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-comment/">Python – Comments, Indentations and Statements</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-random-number/">Python – Random Number</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-variables-and-data-types/">Python – Variables and Data Types</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-variable-scope/">Python – Variable Scope</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/identifiers-in-python/">Python – Identifiers</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-number-types-conversion/">Python – Number Types</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-string/">Python – Strings</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-interpreter/">Python – Interpreter</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-operator/">Python – Operators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-bitwise-operators/">Python – Bitwise Operators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-comparison-operators/">Python – Comparison Operators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-operator-overloading/">Python – Operator Overloading</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-ternary-operator/">Python – Ternary Operator</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-operator-precedence/">Python – Operator Precedence</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-namespace-and-variable-scope/">Python – Namespaces</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-decision-making-expressions/">Python – Decision Making</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-switch-case/">Python – Implement Switch Case</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-data-structures-tutorial/">Python – Data Structures</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-list-examples/">Python – Lists</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-tuple/">Python – Tuples</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-set-and-booleans-with-examples/">Python – Sets &amp; Booleans</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-list-comprehension/">Python – List Comprehension</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-loop/">Python – Loops in Python</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-function/">Python – Functions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-function-arguments/">Python – Function Arguments</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-built-in-functions/">Python – Built-in Functions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-range-function/">Python – Range() Function</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-zip-function/">Python – Zip Function</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-eval-function/">Python – Eval Function</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-exec-function/">Python – exec Function</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-repr-function/">Python – repr Function</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-collections-module/">Python – Collections Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-counter/">Python – Counters</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-namedtuple/">Python – Namedtuples</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-dictionary/">Python – Dictionaries</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-defaultdict/">Python – DefaultDict</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-ordereddict/">Python – OrderedDict</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-datetime-module/">Python – DateTime</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-modules/">Python – Modules</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-pickle/">Python – Serialization</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-packages/">Python – Packages</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-os-module/">Python – Python OS Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-pprint/">Python – Python pprint Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-virtual-environment/">Python – Virtual Environment</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-date-and-time/">Python – Date and Time</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-calendar-module/">Python – Calendar Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-recursion-function/">Python – Recursion</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-lambda-expression/">Python – Lambda Expression</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-decorator/">Python – Decorators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-generator/">Python – Generators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-iterator/">Python – Iterators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-closure/">Python – Closures</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-class/">Python – Classes</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-method/">Python – Methods</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-constructor/">Python – Constructors</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-object/">Python – Object</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-inheritance/">Python – Inheritance</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-multiple-inheritance/">Python – Multiple Inheritance</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-compilers/">Python – Compilers &amp; Interpreters</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-zipfile/">Python – ZipFile</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-file/">Python – File I/O</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/file-handling-in-python/">Python – File Handling</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/how-python-copy-a-file/">Python – Copy A File</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/copy-in-python-deep-shallow-copy/">Python – Shallow &amp; Deep Copy</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-rename-file/">Python – Rename A File</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-exception/">Python – Errors and Exceptions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-exception-handling/">Python – Exception Handling</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-assert/">Python – Assert Statements</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-directory/">Python – Directories</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-itertools/">Python – Iterables</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-itertools-tutorial/">Python – Itertool</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-property-problem-solution/">Python – Property</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-sequence/">Python – Sequences and Collections</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-multithreading/">Python – Multithreading</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-regex-tutorial/">Python – Regular Expressions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-debugger/">Python – Debugger</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-multiprocessing/">Python – Multi Processing</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-xml-parser/">Python – XML Processing</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-cgi-programming/">Python – CGI Programming</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-libraries/">Python – Library</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-math-library/">Python – Math Libraries</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/scipy-tutorial/">Python – SciPy</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-numpy-tutorial/">Python – NumPy</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/pandas-tutorial/">Python – Pandas</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-pyqt5-tutorial/">Python – PyQT</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-array-module/">Python – Array Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-database-access/">Python – Database Access</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-3-extension-programming/">Python – Programming with C</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-web-framework/">Python – Frameworks</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-flask-tutorial/">Python – Flask</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-django-tutorial/">Python – Django</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-forensics/">Python – Forensics</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-network-programming/">Python – Network Programming</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/image-processing-with-scipy-and-numpy/">Python – Image Processing </a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-send-email/">Python – Sending Email</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-gui-programming/">Python – GUI Programming</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-unittest/">Python – Unittest</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-logging/">Python – Logging</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-slice/">Python – Slice</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-subprocess-module/">Python – Subprocess Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-sys-module/">Python – sys Module</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-glossary/">Python – Terminologies Part 1</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-glossary-of-terms/">Python – Terminologies Part 2</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/opencv-python-tutorial/">Python – OpenCV &amp; Computer Vision</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/opencv-features/">Python – OpenCV Features</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/install-opencv-on-ubuntu/">Python – OpenCV Environment Setup</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/read-display-save-image-opencv/">Python – Read, Display &amp; Save Image in OpenCV</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/computer-vision-techniques/">Python – Computer Vision Techniques</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/why-should-i-learn-python/">Python – Reasons Why Learn Python</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/best-python-book/">Python – Best Python Books</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-applications/">Python – Applications</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-in-healthcare/">Python – Healthcare</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-for-stock-market/">Python – Stock Market</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-case-studies/">Python – Case Studies</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-at-netflix/">Case Study – Python at Netflix</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-tuples-vs-lists/">Python – Tuples vs Lists</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-modules-vs-packages/">Python – Modules vs Packages</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-generator-vs-iterator/">Python – Generators vs Iterators</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-method-and-function/">Python – Methods vs Functions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/scala-vs-python/">Python vs Scala</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-vs-java/">Python vs Java</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/r-vs-python/">Python vs R</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/start-learning-python-with-infographic/">Python For Beginners – Infographic</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-features-infographic/">Python Features – Infographic</a></li></ul></div></div><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python"><span class="dfacc-label-title">Python for Data Science</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-for-data-science/">Learn Python for Data Science</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/data-science-tutorial/">Python – Data Science Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/learn-python-for-data-science/">Mastering Python for Data Science</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-data-science-environment-setup/">Python – Data Science Installation</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-data-cleansing/">Python – Data Cleansing &amp; Operations</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-data-file-formats/">Python – Data File Formats</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/relational-database-with-python/">Python – Relational Database</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/nosql-database-in-python/">Python – NoSQL Database</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-stemming/">Python – Stemming &amp; Lemmatization</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/data-wrangling-with-python/">Python – Aggregation &amp; Data Wrangling</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-statistics/">Python – Statistics</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-descriptive-statistics/">Python – Descriptive Statistics</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-probability-distributions/">Python – Probability Distributions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-anaconda-tutorial/">Python Anaconda Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-matplotlib-tutorial/">Python – Matplotlib</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-scatter-plot/">Python – Scatter &amp; Box Plots</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-charts/">Python – Bubble &amp; 3D Charts</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-heatmap-word-cloud/">Python – Heatmap &amp; Word Cloud</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-histogram-python-bar/">Python – Histogram &amp; Bar Plot</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-geographic-maps-graph-data/">Python – Geographical Map &amp; Graph</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-time-series/">Python – Time Series</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-linear-regression-chi-square-test/">Python – Linear Regression</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-for-machine-learning/">Python – Importance for ML</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-machine-learning-tutorial/">Python ML – Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-machine-learning-environment-setup/">Python ML – Environment Setup</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-ml-data-preprocessing/">Python ML – Data Pre-Processing</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/train-test-set-in-python-ml/">Python ML – Train and Test Set</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-machine-learning-techniques/">Python ML – Techniques</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/applications-of-machine-learning/">Python ML – Application</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/machine-learning-algorithms-in-python/">Python ML – Algorithms</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/deep-learning-with-python-tutorial/">Python Deep Learning Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-deep-learning-environment-setup/">Python DL – Environment Setup</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/applications-of-deep-learning-with-python/">Python DL – Application</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/deep-learning-with-python-libraries/">Python DL – Python Libraries</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/deep-neural-networks-with-python/">Python DL – Deep Neural Networks</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/computational-graphs-deep-learning-python/">Python DL – Computational Graphs</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-ai-tutorial/">Python AI Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/nltk-python-tutorial/">Python AI – NLTK</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-speech-recognition-ai/">Python AI – Speech Recognition</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/nlp-tutorial-natural-language-processing/">Python AI – NLP Tutorial</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/heuristic-search-ai/">Python AI – Heuristic Search</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-genetic-algorithms-ai/">Python AI – Genetic Algorithms</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/ai-python-computer-vision/">Python AI – Computer Vision</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-logic-programming/">Python AI – Logic Programming</a></li></ul></div></div><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python"><span class="dfacc-label-title">Python Career Guides</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/facts-about-python-programming/">Python – Interesting Facts</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/why-is-python-in-demand/">Python – Demand</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-future/">Python – Future</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-career-opportunities/">Python – Career Opportunities</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/how-to-get-a-job-in-python-as-a-fresher/">Python – How Fresher Gets Job</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/job-python-resume/">Python – Create Resume</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-career-path/">Python – Career Path</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/how-to-become-a-python-developer/">Python – Become Developer</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-career-infographic/">Python Career – Infographic</a></li></ul></div></div><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python df-acc-lnew"><span class="dfacc-label-title">Python Projects</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli dfblognew"><a href="https://data-flair.training/blogs/python-projects-with-source-code/">Python – Top Projects</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-project-ideas/">Python – Project Ideas</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/computer-vision-project-ideas/">Python – Computer Vision Project Ideas</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/advanced-python-project-detecting-fake-news/">Python Project- Detecting Fake News</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-project-music-genre-classification/">Python Project- Music Genre Classification</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-machine-learning-project-detecting-parkinson-disease/">Python Project- Detecting Parkinson’s Disease</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/project-in-python-colour-detection/">Python Project- Color Detection</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/stock-price-prediction-machine-learning-project-in-python/">Python Project- Stock Price Prediction</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/">Python Project- Speech Emotion Recognition</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/project-in-python-breast-cancer-classification/">Python Project- Breast Cancer Classification</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-project-gender-age-detection/">Python Project- Gender &amp; Age Detection</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-deep-learning-project-handwritten-digit-recognition/">Python Project- Handwritten Digit Recognition</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-chatbot-project/">Python Project- Chatbot</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-project-driver-drowsiness-detection-system/">Python Project- Drowsiness Detection System</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-project-traffic-signs-recognition/">Python Project- Traffic Signs Recognition</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/">Python Project- Image Caption Generator</a></li><li class="dfacc-lsbli dfblognew"><a href="https://data-flair.training/blogs/machine-learning-datasets/">70+ Project Ideas &amp; Datasets</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-open-source-projects/">Python – 56 Open-source Projects</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-game-project-ideas/">Python – Game Project Ideas</a></li></ul></div></div><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python"><span class="dfacc-label-title">Python Interview Questions</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/top-python-interview-questions-answer/">Python – Beginners Interview Questions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-interview-questions/">Python – Intermediates Interview Questions</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-programming-interview-questions/">Python – Experts Interview Questions </a></li></ul></div></div><div class="df-accordion-panel"><div class="df-accordion-label df-acc-python"><span class="dfacc-label-title">Python Quiz</span></div><div class="df-accordion-panel-content"><ul class="dfacc-lsbul"><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-quiz/">Python Quiz- Part 1</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-mcq/">Python Quiz- Part 2</a></li><li class="dfacc-lsbli"><a href="https://data-flair.training/blogs/python-online-quiz/">Python Quiz- Part 3</a></li></ul></div></div></div></div></div></div><div class="sidebar s2 collapsed" data-position="right" data-layout="col-3cm" data-sb-id="s2"><a class="sidebar-toggle" title="Expand Sidebar"><i class="fas icon-sidebar-toggle"></i></a><div class="sidebar-content"><div id="search-5" class="widget widget_search"><form method="get" class="searchform themeform" action="https://data-flair.training/blogs/"><div>
<input type="text" class="search" name="s" onblur="if(this.value==&#39;&#39;)this.value=&#39;To search type and hit enter&#39;;" onfocus="if(this.value==&#39;To search type and hit enter&#39;)this.value;" value="To search type and hit enter"></div></form></div><div id="text-8" class="widget widget_text"><div class="textwidget"><div id="ad-test1"></div></div></div><div id="text-9" class="widget widget_text"><div class="textwidget"><div id="ad-test2"></div></div></div></div></div></div></div></div></div><footer id="footer"><section class="container" id="footer-full-width-widget"><div class="container-inner"><div id="text-2" class="widget widget_text"><div class="textwidget"><p><a href="https://data-flair.training/">Home</a> <a href="https://data-flair.training/about-us/">About us</a> <a href="https://data-flair.training/contact-us/">Contact us</a> <a href="https://data-flair.training/terms-and-conditions/">Terms and Conditions</a> <a href="https://data-flair.training/cancellation-and-refund/">Cancellation and Refund</a> <a href="https://data-flair.training/privacy-policy/">Privacy Policy</a> <a href="https://data-flair.training/disclaimer/">Disclaimer</a> <a href="https://data-flair.training/blogs/">Blogs</a> <a href="https://data-flair.training/blogs/write-for-us/">Write For Us</a> <a href="https://data-flair.training/careers/">Careers</a> <a href="https://data-flair.training/testimonials/">Success Stories</a></p></div></div><div id="custom_html-7" class="widget_text widget widget_custom_html"><div class="textwidget custom-html-widget"><div id="dfsf-prev-banner"><div class="banner-title-holder"></div>
<span class="banner-title"></span></div><div id="dfsf-next-banner"><div class="banner-title-holder"></div>
<span class="banner-title"></span></div><div class="df-sticky-footer"><div class="dfsf-progress-bar"></div><div class="dfsf-category-link"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#"></a></div>
<span class="dfsf-post-details"></span><div class="dfsf-next-prev-buttons"><div class="dfsf-prev"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#">❮ Prev</a></div><div class="dfsf-next"><a href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#">Next ❯</a></div></div></div></div></div></div></section><section class="container" id="footer-widgets"><div class="container-inner"><div class="pad group"><div class="footer-widget-1 grid one-third "><div id="text-7" class="widget widget_text"><div class="textwidget"><div id="dfaboutfooter"><p><span style="font-size: 1.1em; font-weight: bold;">Popular Courses</span></p><p><a href="https://data-flair.training/hadoop-spark-developer-course/"><img id="df-sprite-hdsp" class="alignnone size-full wp-image-45804" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Hadoop Tutorials logo">Hadoop + Spark Course</a><br>
<a href="https://data-flair.training/big-data-hadoop/"><img id="df-sprite-hadoop" class="alignnone size-full wp-image-45804" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Hadoop Tutorials logo">Big Data Hadoop Course</a><br>
<a href="https://data-flair.training/apache-spark-scala/"><img id="df-sprite-spark" class="alignnone size-full wp-image-45809" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Spark Tutorials logo">Spark Scala Course</a><br>
<a href="https://data-flair.training/python-course/"><img id="df-sprite-py" class="alignnone size-full wp-image-45810" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Python Course logo">Python Course</a></p></div></div></div></div><div class="footer-widget-2 grid one-third "><div id="text-4" class="widget widget_text"><div class="textwidget"><div id="dfcategoryfooter1"><p><span style="font-size: 1.1em; font-weight: bold;">Popular Tutorials</span></p><p><a href="https://data-flair.training/blogs/hadoop-tutorials-home/"><img id="df-sprite-had" class="alignnone size-full wp-image-45804" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Hadoop Tutorials logo">Hadoop Tutorials</a><br>
<a href="https://data-flair.training/blogs/spark-tutorials-home/"><img id="df-sprite-sp" class="alignnone size-full wp-image-45809" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Spark Tutorials logo">Spark Tutorials</a><br>
<a href="https://data-flair.training/blogs/flink-tutorials-home/"><img id="df-sprite-fli" class="alignnone size-full wp-image-45810" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Flink Tutorials logo">Flink Tutorials</a><br>
<a href="https://data-flair.training/blogs/tableau-tutorials-home/"><img id="df-sprite-tab" class="alignnone size-full wp-image-45811" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Tableau Tutorials logo">Tableau Tutorials</a><br>
<a href="https://data-flair.training/blogs/power-bi-tutorials-home/"><img id="df-sprite-pbi" class="alignnone size-full wp-image-45812" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Power BI Tutorials logo">Power BI Tutorials</a></p></div></div></div></div><div class="footer-widget-3 grid one-third last"><div id="text-5" class="widget widget_text"><div class="textwidget"><div id="dfcategoryfooter2"><p><span style="font-size: 1.1em; font-weight: bold;">Popular Tutorials</span></p><p><a href="https://data-flair.training/blogs/data-science-tutorials-home/"><img id="df-sprite-ds" class="alignnone size-full wp-image-45814" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Data Science Tutorials logo">Data Science Tutorials</a><br>
<a href="https://data-flair.training/blogs/machine-learning-tutorials-home/"><img id="df-sprite-ml" class="alignnone size-full wp-image-45815" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Machine Learning Tutorials logo">Machine Learning Tutorials</a><br>
<a href="https://data-flair.training/blogs/python-tutorials-home/"><img id="df-sprite-py" class="alignnone size-full wp-image-45816" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="Python Tutorials logo">Python Tutorials</a><br>
<a href="https://data-flair.training/blogs/r-tutorials-home/"><img id="df-sprite-r" class="alignnone size-full wp-image-45817" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="R Tutorials logo">R Tutorials</a><br>
<a href="https://data-flair.training/blogs/sas-tutorials-home/"><img id="df-sprite-sas" class="alignnone size-full wp-image-45818" style="vertical-align: middle; padding-right: 7px; padding-bottom: 4px;" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/transparent-01.png" alt="SAS Tutorials logo">SAS Tutorials</a></p></div></div></div></div></div></div></section><section class="container" id="footer-bottom"><div class="container-inner"><a id="back-to-top" href="https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/#"><i class="fas fa-angle-up"></i></a><div class="pad group"><div class="grid one-half">
<img id="footer-logo" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/DataFlair_Logo_Final-01.png" alt=""><div id="copyright"><p>DataFlair © 2020. All Rights Reserved.</p></div></div><div class="grid one-half last"><ul class="social-links"><li><a rel="nofollow" class="social-tooltip" title="Like us on Facebook" aria-label="Like us on Facebook" href="https://www.facebook.com/DataFlairWS/" target="_blank"><i class="fab fa-facebook"></i></a></li><li><a rel="nofollow" class="social-tooltip" title="Read us on Twitter" aria-label="Read us on Twitter" href="https://twitter.com/DataFlairWS" target="_blank"><i class="fab fa-twitter"></i></a></li><li><a rel="nofollow" class="social-tooltip" title="Follow us on LinkedIn" aria-label="Follow us on LinkedIn" href="https://www.linkedin.com/company/dataflair-web-services-pvt-ltd/" target="_blank"><i class="fab fa-linkedin"></i></a></li><li><a rel="nofollow" class="social-tooltip" title="Watch us on Youtube" aria-label="Watch us on Youtube" href="https://www.youtube.com/user/DataFlairWS" target="_blank"><i class="fab fa-youtube"></i></a></li><li><a rel="nofollow" class="social-tooltip" title="Write to us" aria-label="Write to us" href="mailto:info@data-flair.training" target="_blank"><i class="fas fa-envelope"></i></a></li></ul></div></div></div></section></footer></div> <script>setTimeout (function() {Array.prototype.forEach.call (document.querySelectorAll (".ai-viewports"), function (element, index) {ai_insert_code (element);});}, 10);</script> <script>var WfcFrontParams = {"effectsAndIconsSelectorCandidates":[],"wfcOptions":null};</script> <script>var mPS2id_params = {"instances":{"mPS2id_instance_0":{"selector":"a[href*=#]:not([href=#])","autoSelectorMenuLinks":"true","scrollSpeed":800,"autoScrollSpeed":"true","scrollEasing":"easeInOutQuint","scrollingEasing":"easeOutQuint","pageEndSmoothScroll":"true","stopScrollOnUserAction":"false","autoCorrectScroll":"true","layout":"vertical","offset":"130","highlightSelector":"","clickedClass":"mPS2id-clicked","targetClass":"mPS2id-target","highlightClass":"mPS2id-highlight","forceSingleHighlight":"false","keepHighlightUntilNext":"false","highlightByNextTarget":"false","appendHash":"false","scrollToHash":"true","scrollToHashForAll":"true","scrollToHashDelay":0,"scrollToHashUseElementData":"true","scrollToHashRemoveUrlHash":"true","disablePluginBelow":0,"adminDisplayWidgetsId":"true","adminTinyMCEbuttons":"true","unbindUnrelatedClickEvents":"false","normalizeAnchorPointTargets":"false"}},"total_instances":"1","shortcode_class":"_ps2id"};</script> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/underscore.min.js.download"></script> <script>var HUParams = {"_disabled":[],"SmoothScroll":{"Enabled":false,"Options":{"touchpadSupport":false}},"centerAllImg":"1","timerOnScrollAllBrowsers":"1","extLinksStyle":"","extLinksTargetExt":"","extLinksSkipSelectors":{"classes":["btn","button"],"ids":[]},"imgSmartLoadEnabled":"1","imgSmartLoadOpts":{"parentSelectors":[".container .content",".container .sidebar","#footer","#header-widgets"],"opts":{"excludeImg":[".tc-holder-img"],"fadeIn_options":100}},"goldenRatio":"1.618","gridGoldenRatioLimit":"350","sbStickyUserSettings":{"desktop":false,"mobile":false},"isWPMobile":"","menuStickyUserSettings":{"desktop":"stick_up","mobile":"stick_up"},"isDevMode":"","ajaxUrl":"https:\/\/data-flair.training\/blogs\/?huajax=1","frontNonce":{"id":"HuFrontNonce","handle":"4e11151e58"},"userStarted":{"with":"before|1.1.3","on":{"date":"2018-09-08 10:06:27.000000","timezone_type":3,"timezone":"UTC"}},"isWelcomeNoteOn":"","welcomeContent":"","fitTextMap":{"single_post_title":{"selectors":".single h1.entry-title","minEm":1.375,"maxEm":2.62},"page_title":{"selectors":".page-title h1","minEm":1,"maxEm":1.3},"home_page_title":{"selectors":".home .page-title","minEm":1,"maxEm":1.2,"compression":2.5},"post_titles":{"selectors":".blog .post-title, .archive .post-title","minEm":1.375,"maxEm":1.475},"featured_post_titles":{"selectors":".featured .post-title","minEm":1.375,"maxEm":2.125},"comments":{"selectors":".commentlist li","minEm":0.8125,"maxEm":0.93,"compression":2.5},"entry":{"selectors":".entry","minEm":0.9375,"maxEm":1.125,"compression":2.5},"content_h1":{"selectors":".entry h1, .woocommerce div.product h1.product_title","minEm":1.7578125,"maxEm":2.671875},"content_h2":{"selectors":".entry h2","minEm":1.5234375,"maxEm":2.390625},"content_h3":{"selectors":".entry h3","minEm":1.40625,"maxEm":1.96875},"content_h4":{"selectors":".entry h4","minEm":1.2890625,"maxEm":1.6875},"content_h5":{"selectors":".entry h5","minEm":1.0546875,"maxEm":1.40625},"content_h6":{"selectors":".entry h6","minEm":0.9375,"maxEm":1.265625,"compression":2.5}},"userFontSize":"16","fitTextCompression":"1.5"};</script> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/autoptimize_single_8decb0ce7eb8097d463a77d859a02725.js.download"></script> <script src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/EnlighterJS.min.js.download"></script> <script>var icegram_pre_data = {"ajax_url":"https:\/\/data-flair.training\/blogs\/wp-admin\/admin-ajax.php","post_obj":{"is_home":false,"page_id":72771,"action":"display_messages","shortcodes":[],"cache_compatibility":"yes","device":"laptop"}};</script> <script data-cfasync="false" async="async" defer="defer" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/api.js.download"></script> <script>var gglcptch = {"options":{"version":"v3","sitekey":"6LdD0JwUAAAAAABRdMQUEDhUkj_9jYOOLjwoOiqV","theme":"red","error":"<strong>Warning<\/strong>:&nbsp;More than one reCAPTCHA has been found in the current form. Please remove all unnecessary reCAPTCHA fields to make it work properly."},"vars":{"visibility":false}};</script> <!--[if lt IE 9]> <script src=https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/themes/hueman-pro/assets/front/js/ie/respond.js></script> <![endif]--> <script>EnlighterJS_Config = {"selector":{"block":"pre.EnlighterJSRAW","inline":"code.EnlighterJSRAW"},"language":"generic","theme":"eclipse","indent":2,"hover":"hoverEnabled","showLinenumbers":true,"rawButton":false,"infoButton":false,"windowButton":false,"rawcodeDoubleclick":false,"grouping":true,"cryptex":{"enabled":false,"email":"mail@example.tld"}};!function(){var a=function(a){var b="Enlighter Error: ";console.error?console.error(b+a):console.log&&console.log(b+a)};return window.addEvent?"undefined"==typeof EnlighterJS?void a("Javascript Resources not loaded yet!"):"undefined"==typeof EnlighterJS_Config?void a("Configuration not loaded yet!"):void window.addEvent("domready",function(){EnlighterJS.Util.Init(EnlighterJS_Config.selector.block,EnlighterJS_Config.selector.inline,EnlighterJS_Config)}):void a("MooTools Framework not loaded yet!")}();;</script><script defer="" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/autoptimize_ea0f8278c3b94670002fde2b8868dbeb.js.download"></script>
<!--
Performance optimized by W3 Total Cache. Learn more: https://www.w3-edge.com/products/

Object Caching 0/0 objects using disk
Page Caching using disk: enhanced 
Content Delivery Network via d2h0cx97tjks2p.cloudfront.net
Minified using disk (Request URI is rejected)
Database Caching 131/163 queries in 0.027 seconds using disk (Request-wide modification query)

Served from: data-flair.training @ 2020-05-31 12:33:19 by W3 Total Cache
--><ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:0px;margin:0;padding:0;position:relative;visibility:visible;width:0px;background-color:transparent;"><ins id="aswift_0_anchor" style="display:block;border:none;height:0px;margin:0;padding:0;position:relative;visibility:visible;width:0px;background-color:transparent;"><iframe id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;border:0;width:undefinedpx;height:undefinedpx;" sandbox="allow-forms allow-pointer-lock allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" frameborder="0" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/ads.html" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" data-google-container-id="a!0" data-load-complete="true"></iframe></ins></ins></ins><ins class="adsbygoogle adsbygoogle-noablate" data-adsbygoogle-status="done" style="display: none !important;"></ins><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/anchor.html" width="256" height="60" role="presentation" name="a-phag40l55k9b" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div></div><div><div class="grecaptcha-badge" data-style="none" style="width: 256px; height: 60px; position: fixed; visibility: hidden;"><div class="grecaptcha-logo"><iframe src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/anchor(1).html" width="256" height="60" role="presentation" name="a-3blmv3c0t4aj" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100001" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;"></iframe></div><iframe id="google_osd_static_frame_2915528956709" name="google_osd_static_frame" style="display: none; width: 0px; height: 0px;"></iframe><iframe src="https://tpc.googlesyndication.com/sodar/sodar2/209/runner.html" width="0" height="0" style="display: none;"></iframe></body><iframe id="google_esf" name="google_esf" src="./Python based Project - Learn to Build Image Caption Generator with CNN &amp; LSTM - DataFlair_files/zrt_lookup.html" data-ad-client="ca-pub-9834688387117374" style="display: none;"></iframe></html>